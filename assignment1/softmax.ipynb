{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.323671\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *The total loss is roughly the average sample loss, where individual sample losses are equal to -log(exp(scores[y[i]]/sum exp(scores[j]))).  This expression is approximately equal to -log(0.1), because there are 10 classes and exp(scores[j]) is approximately equal for every class j*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 3.835330 analytic: 3.835330, relative error: 1.565244e-08\n",
      "numerical: -0.841135 analytic: -0.841135, relative error: 5.598817e-08\n",
      "numerical: 1.300883 analytic: 1.300883, relative error: 3.256073e-08\n",
      "numerical: 0.987443 analytic: 0.987443, relative error: 1.587662e-08\n",
      "numerical: -2.934114 analytic: -2.934114, relative error: 3.011295e-09\n",
      "numerical: -1.096781 analytic: -1.096782, relative error: 3.456317e-08\n",
      "numerical: 1.794750 analytic: 1.794750, relative error: 4.685653e-08\n",
      "numerical: -2.588508 analytic: -2.588508, relative error: 4.081026e-09\n",
      "numerical: 1.048158 analytic: 1.048158, relative error: 2.683143e-08\n",
      "numerical: -0.065973 analytic: -0.065973, relative error: 4.825376e-07\n",
      "numerical: 3.101299 analytic: 3.101299, relative error: 1.280395e-08\n",
      "numerical: 0.018532 analytic: 0.018532, relative error: 4.545814e-07\n",
      "numerical: -1.885942 analytic: -1.885942, relative error: 3.107082e-08\n",
      "numerical: -1.243170 analytic: -1.243170, relative error: 1.598561e-08\n",
      "numerical: 2.543264 analytic: 2.543264, relative error: 1.353100e-08\n",
      "numerical: 0.828964 analytic: 0.828964, relative error: 3.355977e-08\n",
      "numerical: -2.436733 analytic: -2.436733, relative error: 1.230687e-08\n",
      "numerical: -1.074415 analytic: -1.074415, relative error: 4.252532e-08\n",
      "numerical: 1.620346 analytic: 1.620346, relative error: 3.093163e-08\n",
      "numerical: 3.207595 analytic: 3.207595, relative error: 9.223282e-09\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.323671e+00 computed in 0.074101s\n",
      "vectorized loss: 2.323671e+00 computed in 0.012345s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr, rs:  1e-07 1000.0\n",
      "iteration 0 / 20000: loss 21.572121\n",
      "iteration 100 / 20000: loss 19.208301\n",
      "iteration 200 / 20000: loss 18.024268\n",
      "iteration 300 / 20000: loss 17.993811\n",
      "iteration 400 / 20000: loss 17.437061\n",
      "iteration 500 / 20000: loss 17.039108\n",
      "iteration 600 / 20000: loss 16.446650\n",
      "iteration 700 / 20000: loss 15.975938\n",
      "iteration 800 / 20000: loss 16.000173\n",
      "iteration 900 / 20000: loss 15.780656\n",
      "iteration 1000 / 20000: loss 15.461533\n",
      "iteration 1100 / 20000: loss 14.908549\n",
      "iteration 1200 / 20000: loss 14.598197\n",
      "iteration 1300 / 20000: loss 14.122927\n",
      "iteration 1400 / 20000: loss 14.135792\n",
      "iteration 1500 / 20000: loss 13.863384\n",
      "iteration 1600 / 20000: loss 13.273181\n",
      "iteration 1700 / 20000: loss 13.303626\n",
      "iteration 1800 / 20000: loss 12.934205\n",
      "iteration 1900 / 20000: loss 12.658559\n",
      "iteration 2000 / 20000: loss 12.554081\n",
      "iteration 2100 / 20000: loss 12.140032\n",
      "iteration 2200 / 20000: loss 12.252253\n",
      "iteration 2300 / 20000: loss 11.823265\n",
      "iteration 2400 / 20000: loss 11.420085\n",
      "iteration 2500 / 20000: loss 11.284257\n",
      "iteration 2600 / 20000: loss 11.115088\n",
      "iteration 2700 / 20000: loss 10.966224\n",
      "iteration 2800 / 20000: loss 10.742059\n",
      "iteration 2900 / 20000: loss 10.553110\n",
      "iteration 3000 / 20000: loss 10.494758\n",
      "iteration 3100 / 20000: loss 10.361367\n",
      "iteration 3200 / 20000: loss 9.934895\n",
      "iteration 3300 / 20000: loss 9.743239\n",
      "iteration 3400 / 20000: loss 9.699558\n",
      "iteration 3500 / 20000: loss 9.769738\n",
      "iteration 3600 / 20000: loss 9.262231\n",
      "iteration 3700 / 20000: loss 9.388525\n",
      "iteration 3800 / 20000: loss 8.968208\n",
      "iteration 3900 / 20000: loss 8.844977\n",
      "iteration 4000 / 20000: loss 8.746687\n",
      "iteration 4100 / 20000: loss 8.562749\n",
      "iteration 4200 / 20000: loss 8.409867\n",
      "iteration 4300 / 20000: loss 8.177173\n",
      "iteration 4400 / 20000: loss 8.323888\n",
      "iteration 4500 / 20000: loss 8.075293\n",
      "iteration 4600 / 20000: loss 7.994683\n",
      "iteration 4700 / 20000: loss 7.889005\n",
      "iteration 4800 / 20000: loss 7.751797\n",
      "iteration 4900 / 20000: loss 7.465071\n",
      "iteration 5000 / 20000: loss 7.367004\n",
      "iteration 5100 / 20000: loss 7.266348\n",
      "iteration 5200 / 20000: loss 7.273447\n",
      "iteration 5300 / 20000: loss 7.202125\n",
      "iteration 5400 / 20000: loss 6.953690\n",
      "iteration 5500 / 20000: loss 6.918260\n",
      "iteration 5600 / 20000: loss 6.738025\n",
      "iteration 5700 / 20000: loss 6.702260\n",
      "iteration 5800 / 20000: loss 6.440164\n",
      "iteration 5900 / 20000: loss 6.363724\n",
      "iteration 6000 / 20000: loss 6.198972\n",
      "iteration 6100 / 20000: loss 6.376138\n",
      "iteration 6200 / 20000: loss 6.198444\n",
      "iteration 6300 / 20000: loss 6.021471\n",
      "iteration 6400 / 20000: loss 5.980861\n",
      "iteration 6500 / 20000: loss 5.901046\n",
      "iteration 6600 / 20000: loss 5.893092\n",
      "iteration 6700 / 20000: loss 5.698401\n",
      "iteration 6800 / 20000: loss 5.542507\n",
      "iteration 6900 / 20000: loss 5.591214\n",
      "iteration 7000 / 20000: loss 5.558687\n",
      "iteration 7100 / 20000: loss 5.428384\n",
      "iteration 7200 / 20000: loss 5.421495\n",
      "iteration 7300 / 20000: loss 5.289540\n",
      "iteration 7400 / 20000: loss 5.261221\n",
      "iteration 7500 / 20000: loss 5.131197\n",
      "iteration 7600 / 20000: loss 5.086698\n",
      "iteration 7700 / 20000: loss 5.000206\n",
      "iteration 7800 / 20000: loss 4.914332\n",
      "iteration 7900 / 20000: loss 4.800727\n",
      "iteration 8000 / 20000: loss 4.870130\n",
      "iteration 8100 / 20000: loss 4.817002\n",
      "iteration 8200 / 20000: loss 4.640446\n",
      "iteration 8300 / 20000: loss 4.625682\n",
      "iteration 8400 / 20000: loss 4.619670\n",
      "iteration 8500 / 20000: loss 4.395505\n",
      "iteration 8600 / 20000: loss 4.558941\n",
      "iteration 8700 / 20000: loss 4.436526\n",
      "iteration 8800 / 20000: loss 4.346044\n",
      "iteration 8900 / 20000: loss 4.269949\n",
      "iteration 9000 / 20000: loss 4.286125\n",
      "iteration 9100 / 20000: loss 4.131067\n",
      "iteration 9200 / 20000: loss 4.196630\n",
      "iteration 9300 / 20000: loss 4.064195\n",
      "iteration 9400 / 20000: loss 4.086344\n",
      "iteration 9500 / 20000: loss 3.980372\n",
      "iteration 9600 / 20000: loss 3.872583\n",
      "iteration 9700 / 20000: loss 3.855666\n",
      "iteration 9800 / 20000: loss 3.860106\n",
      "iteration 9900 / 20000: loss 3.782069\n",
      "iteration 10000 / 20000: loss 3.882396\n",
      "iteration 10100 / 20000: loss 3.816318\n",
      "iteration 10200 / 20000: loss 3.751076\n",
      "iteration 10300 / 20000: loss 3.638087\n",
      "iteration 10400 / 20000: loss 3.697406\n",
      "iteration 10500 / 20000: loss 3.472179\n",
      "iteration 10600 / 20000: loss 3.620851\n",
      "iteration 10700 / 20000: loss 3.503067\n",
      "iteration 10800 / 20000: loss 3.331396\n",
      "iteration 10900 / 20000: loss 3.394633\n",
      "iteration 11000 / 20000: loss 3.512823\n",
      "iteration 11100 / 20000: loss 3.397725\n",
      "iteration 11200 / 20000: loss 3.423137\n",
      "iteration 11300 / 20000: loss 3.280912\n",
      "iteration 11400 / 20000: loss 3.216515\n",
      "iteration 11500 / 20000: loss 3.285799\n",
      "iteration 11600 / 20000: loss 3.129102\n",
      "iteration 11700 / 20000: loss 3.206119\n",
      "iteration 11800 / 20000: loss 3.131686\n",
      "iteration 11900 / 20000: loss 3.011304\n",
      "iteration 12000 / 20000: loss 3.210457\n",
      "iteration 12100 / 20000: loss 3.034452\n",
      "iteration 12200 / 20000: loss 3.056470\n",
      "iteration 12300 / 20000: loss 3.064250\n",
      "iteration 12400 / 20000: loss 2.921602\n",
      "iteration 12500 / 20000: loss 2.936747\n",
      "iteration 12600 / 20000: loss 2.938690\n",
      "iteration 12700 / 20000: loss 2.824392\n",
      "iteration 12800 / 20000: loss 2.812984\n",
      "iteration 12900 / 20000: loss 2.965178\n",
      "iteration 13000 / 20000: loss 2.890672\n",
      "iteration 13100 / 20000: loss 2.920129\n",
      "iteration 13200 / 20000: loss 2.684621\n",
      "iteration 13300 / 20000: loss 2.772540\n",
      "iteration 13400 / 20000: loss 2.740738\n",
      "iteration 13500 / 20000: loss 2.758474\n",
      "iteration 13600 / 20000: loss 2.709904\n",
      "iteration 13700 / 20000: loss 2.604962\n",
      "iteration 13800 / 20000: loss 2.680478\n",
      "iteration 13900 / 20000: loss 2.716520\n",
      "iteration 14000 / 20000: loss 2.683518\n",
      "iteration 14100 / 20000: loss 2.665523\n",
      "iteration 14200 / 20000: loss 2.521064\n",
      "iteration 14300 / 20000: loss 2.608125\n",
      "iteration 14400 / 20000: loss 2.591930\n",
      "iteration 14500 / 20000: loss 2.628147\n",
      "iteration 14600 / 20000: loss 2.501588\n",
      "iteration 14700 / 20000: loss 2.514721\n",
      "iteration 14800 / 20000: loss 2.448671\n",
      "iteration 14900 / 20000: loss 2.619884\n",
      "iteration 15000 / 20000: loss 2.543442\n",
      "iteration 15100 / 20000: loss 2.429343\n",
      "iteration 15200 / 20000: loss 2.375738\n",
      "iteration 15300 / 20000: loss 2.489906\n",
      "iteration 15400 / 20000: loss 2.492059\n",
      "iteration 15500 / 20000: loss 2.405142\n",
      "iteration 15600 / 20000: loss 2.309625\n",
      "iteration 15700 / 20000: loss 2.337845\n",
      "iteration 15800 / 20000: loss 2.381768\n",
      "iteration 15900 / 20000: loss 2.383332\n",
      "iteration 16000 / 20000: loss 2.433944\n",
      "iteration 16100 / 20000: loss 2.368025\n",
      "iteration 16200 / 20000: loss 2.386739\n",
      "iteration 16300 / 20000: loss 2.352998\n",
      "iteration 16400 / 20000: loss 2.365391\n",
      "iteration 16500 / 20000: loss 2.240294\n",
      "iteration 16600 / 20000: loss 2.195273\n",
      "iteration 16700 / 20000: loss 2.285989\n",
      "iteration 16800 / 20000: loss 2.156514\n",
      "iteration 16900 / 20000: loss 2.162311\n",
      "iteration 17000 / 20000: loss 2.258384\n",
      "iteration 17100 / 20000: loss 2.304375\n",
      "iteration 17200 / 20000: loss 2.203010\n",
      "iteration 17300 / 20000: loss 2.251187\n",
      "iteration 17400 / 20000: loss 2.197167\n",
      "iteration 17500 / 20000: loss 2.195275\n",
      "iteration 17600 / 20000: loss 2.297357\n",
      "iteration 17700 / 20000: loss 2.183999\n",
      "iteration 17800 / 20000: loss 2.126060\n",
      "iteration 17900 / 20000: loss 2.151101\n",
      "iteration 18000 / 20000: loss 2.244957\n",
      "iteration 18100 / 20000: loss 2.224940\n",
      "iteration 18200 / 20000: loss 2.129656\n",
      "iteration 18300 / 20000: loss 2.061096\n",
      "iteration 18400 / 20000: loss 2.271498\n",
      "iteration 18500 / 20000: loss 2.106809\n",
      "iteration 18600 / 20000: loss 2.137775\n",
      "iteration 18700 / 20000: loss 2.157723\n",
      "iteration 18800 / 20000: loss 2.169145\n",
      "iteration 18900 / 20000: loss 2.109725\n",
      "iteration 19000 / 20000: loss 2.128402\n",
      "iteration 19100 / 20000: loss 2.135677\n",
      "iteration 19200 / 20000: loss 2.165380\n",
      "iteration 19300 / 20000: loss 2.210293\n",
      "iteration 19400 / 20000: loss 2.104519\n",
      "iteration 19500 / 20000: loss 2.075313\n",
      "iteration 19600 / 20000: loss 2.087802\n",
      "iteration 19700 / 20000: loss 1.958832\n",
      "iteration 19800 / 20000: loss 2.043881\n",
      "iteration 19900 / 20000: loss 2.075404\n",
      "lr, rs:  1e-07 5000.0\n",
      "iteration 0 / 20000: loss 82.216038\n",
      "iteration 100 / 20000: loss 73.037958\n",
      "iteration 200 / 20000: loss 65.520918\n",
      "iteration 300 / 20000: loss 59.104758\n",
      "iteration 400 / 20000: loss 53.595703\n",
      "iteration 500 / 20000: loss 48.568738\n",
      "iteration 600 / 20000: loss 44.036986\n",
      "iteration 700 / 20000: loss 39.886329\n",
      "iteration 800 / 20000: loss 36.149726\n",
      "iteration 900 / 20000: loss 33.157432\n",
      "iteration 1000 / 20000: loss 29.988857\n",
      "iteration 1100 / 20000: loss 27.307734\n",
      "iteration 1200 / 20000: loss 24.909431\n",
      "iteration 1300 / 20000: loss 22.602945\n",
      "iteration 1400 / 20000: loss 20.530065\n",
      "iteration 1500 / 20000: loss 18.747652\n",
      "iteration 1600 / 20000: loss 17.001493\n",
      "iteration 1700 / 20000: loss 15.635632\n",
      "iteration 1800 / 20000: loss 14.333472\n",
      "iteration 1900 / 20000: loss 13.147252\n",
      "iteration 2000 / 20000: loss 12.044596\n",
      "iteration 2100 / 20000: loss 11.116219\n",
      "iteration 2200 / 20000: loss 10.306291\n",
      "iteration 2300 / 20000: loss 9.348052\n",
      "iteration 2400 / 20000: loss 8.563573\n",
      "iteration 2500 / 20000: loss 7.995236\n",
      "iteration 2600 / 20000: loss 7.420397\n",
      "iteration 2700 / 20000: loss 6.802057\n",
      "iteration 2800 / 20000: loss 6.480866\n",
      "iteration 2900 / 20000: loss 5.964072\n",
      "iteration 3000 / 20000: loss 5.632706\n",
      "iteration 3100 / 20000: loss 5.295936\n",
      "iteration 3200 / 20000: loss 4.967591\n",
      "iteration 3300 / 20000: loss 4.674342\n",
      "iteration 3400 / 20000: loss 4.344998\n",
      "iteration 3500 / 20000: loss 4.204742\n",
      "iteration 3600 / 20000: loss 3.922871\n",
      "iteration 3700 / 20000: loss 3.734724\n",
      "iteration 3800 / 20000: loss 3.460287\n",
      "iteration 3900 / 20000: loss 3.389555\n",
      "iteration 4000 / 20000: loss 3.207634\n",
      "iteration 4100 / 20000: loss 2.975783\n",
      "iteration 4200 / 20000: loss 2.889631\n",
      "iteration 4300 / 20000: loss 2.789024\n",
      "iteration 4400 / 20000: loss 2.889924\n",
      "iteration 4500 / 20000: loss 2.683280\n",
      "iteration 4600 / 20000: loss 2.669006\n",
      "iteration 4700 / 20000: loss 2.580719\n",
      "iteration 4800 / 20000: loss 2.507838\n",
      "iteration 4900 / 20000: loss 2.576222\n",
      "iteration 5000 / 20000: loss 2.444458\n",
      "iteration 5100 / 20000: loss 2.346488\n",
      "iteration 5200 / 20000: loss 2.373624\n",
      "iteration 5300 / 20000: loss 2.327510\n",
      "iteration 5400 / 20000: loss 2.184973\n",
      "iteration 5500 / 20000: loss 2.201874\n",
      "iteration 5600 / 20000: loss 2.241070\n",
      "iteration 5700 / 20000: loss 2.128823\n",
      "iteration 5800 / 20000: loss 2.092319\n",
      "iteration 5900 / 20000: loss 2.022428\n",
      "iteration 6000 / 20000: loss 2.047131\n",
      "iteration 6100 / 20000: loss 2.039047\n",
      "iteration 6200 / 20000: loss 1.978726\n",
      "iteration 6300 / 20000: loss 2.102876\n",
      "iteration 6400 / 20000: loss 2.106514\n",
      "iteration 6500 / 20000: loss 2.057537\n",
      "iteration 6600 / 20000: loss 1.967515\n",
      "iteration 6700 / 20000: loss 1.902859\n",
      "iteration 6800 / 20000: loss 1.971535\n",
      "iteration 6900 / 20000: loss 1.960992\n",
      "iteration 7000 / 20000: loss 1.871726\n",
      "iteration 7100 / 20000: loss 1.975786\n",
      "iteration 7200 / 20000: loss 1.921082\n",
      "iteration 7300 / 20000: loss 1.988919\n",
      "iteration 7400 / 20000: loss 2.036793\n",
      "iteration 7500 / 20000: loss 1.892945\n",
      "iteration 7600 / 20000: loss 1.888659\n",
      "iteration 7700 / 20000: loss 1.894740\n",
      "iteration 7800 / 20000: loss 1.926618\n",
      "iteration 7900 / 20000: loss 1.921990\n",
      "iteration 8000 / 20000: loss 1.924459\n",
      "iteration 8100 / 20000: loss 1.873284\n",
      "iteration 8200 / 20000: loss 1.869916\n",
      "iteration 8300 / 20000: loss 1.863372\n",
      "iteration 8400 / 20000: loss 1.878757\n",
      "iteration 8500 / 20000: loss 1.862513\n",
      "iteration 8600 / 20000: loss 1.932282\n",
      "iteration 8700 / 20000: loss 1.873290\n",
      "iteration 8800 / 20000: loss 1.942164\n",
      "iteration 8900 / 20000: loss 2.022231\n",
      "iteration 9000 / 20000: loss 1.880432\n",
      "iteration 9100 / 20000: loss 1.956253\n",
      "iteration 9200 / 20000: loss 1.944272\n",
      "iteration 9300 / 20000: loss 1.813273\n",
      "iteration 9400 / 20000: loss 1.809739\n",
      "iteration 9500 / 20000: loss 1.828170\n",
      "iteration 9600 / 20000: loss 1.895425\n",
      "iteration 9700 / 20000: loss 1.954239\n",
      "iteration 9800 / 20000: loss 1.874616\n",
      "iteration 9900 / 20000: loss 1.914581\n",
      "iteration 10000 / 20000: loss 1.956001\n",
      "iteration 10100 / 20000: loss 1.819620\n",
      "iteration 10200 / 20000: loss 1.859915\n",
      "iteration 10300 / 20000: loss 1.820272\n",
      "iteration 10400 / 20000: loss 1.958477\n",
      "iteration 10500 / 20000: loss 1.935211\n",
      "iteration 10600 / 20000: loss 1.885103\n",
      "iteration 10700 / 20000: loss 1.952696\n",
      "iteration 10800 / 20000: loss 1.888904\n",
      "iteration 10900 / 20000: loss 1.875673\n",
      "iteration 11000 / 20000: loss 1.872743\n",
      "iteration 11100 / 20000: loss 1.794942\n",
      "iteration 11200 / 20000: loss 1.833189\n",
      "iteration 11300 / 20000: loss 1.910098\n",
      "iteration 11400 / 20000: loss 1.885863\n",
      "iteration 11500 / 20000: loss 1.851826\n",
      "iteration 11600 / 20000: loss 1.962748\n",
      "iteration 11700 / 20000: loss 1.854801\n",
      "iteration 11800 / 20000: loss 1.933012\n",
      "iteration 11900 / 20000: loss 1.917879\n",
      "iteration 12000 / 20000: loss 1.925353\n",
      "iteration 12100 / 20000: loss 1.928584\n",
      "iteration 12200 / 20000: loss 1.950589\n",
      "iteration 12300 / 20000: loss 1.841734\n",
      "iteration 12400 / 20000: loss 1.899687\n",
      "iteration 12500 / 20000: loss 1.838344\n",
      "iteration 12600 / 20000: loss 1.941711\n",
      "iteration 12700 / 20000: loss 1.946600\n",
      "iteration 12800 / 20000: loss 1.968449\n",
      "iteration 12900 / 20000: loss 1.892131\n",
      "iteration 13000 / 20000: loss 1.835361\n",
      "iteration 13100 / 20000: loss 1.803301\n",
      "iteration 13200 / 20000: loss 1.889669\n",
      "iteration 13300 / 20000: loss 1.944640\n",
      "iteration 13400 / 20000: loss 1.801157\n",
      "iteration 13500 / 20000: loss 1.760471\n",
      "iteration 13600 / 20000: loss 1.929120\n",
      "iteration 13700 / 20000: loss 1.790517\n",
      "iteration 13800 / 20000: loss 1.833344\n",
      "iteration 13900 / 20000: loss 1.949036\n",
      "iteration 14000 / 20000: loss 1.814482\n",
      "iteration 14100 / 20000: loss 1.835321\n",
      "iteration 14200 / 20000: loss 1.887160\n",
      "iteration 14300 / 20000: loss 1.938575\n",
      "iteration 14400 / 20000: loss 1.831080\n",
      "iteration 14500 / 20000: loss 1.880739\n",
      "iteration 14600 / 20000: loss 1.897715\n",
      "iteration 14700 / 20000: loss 1.907900\n",
      "iteration 14800 / 20000: loss 1.973843\n",
      "iteration 14900 / 20000: loss 1.907686\n",
      "iteration 15000 / 20000: loss 1.850506\n",
      "iteration 15100 / 20000: loss 1.857997\n",
      "iteration 15200 / 20000: loss 1.915160\n",
      "iteration 15300 / 20000: loss 1.879992\n",
      "iteration 15400 / 20000: loss 1.870358\n",
      "iteration 15500 / 20000: loss 2.000696\n",
      "iteration 15600 / 20000: loss 1.847157\n",
      "iteration 15700 / 20000: loss 1.893052\n",
      "iteration 15800 / 20000: loss 1.944085\n",
      "iteration 15900 / 20000: loss 1.801168\n",
      "iteration 16000 / 20000: loss 1.788425\n",
      "iteration 16100 / 20000: loss 1.921039\n",
      "iteration 16200 / 20000: loss 1.864018\n",
      "iteration 16300 / 20000: loss 1.848763\n",
      "iteration 16400 / 20000: loss 1.929694\n",
      "iteration 16500 / 20000: loss 1.952561\n",
      "iteration 16600 / 20000: loss 1.873093\n",
      "iteration 16700 / 20000: loss 1.926144\n",
      "iteration 16800 / 20000: loss 1.848654\n",
      "iteration 16900 / 20000: loss 1.758680\n",
      "iteration 17000 / 20000: loss 1.879857\n",
      "iteration 17100 / 20000: loss 1.876191\n",
      "iteration 17200 / 20000: loss 1.891361\n",
      "iteration 17300 / 20000: loss 1.843414\n",
      "iteration 17400 / 20000: loss 1.884093\n",
      "iteration 17500 / 20000: loss 1.884045\n",
      "iteration 17600 / 20000: loss 1.864991\n",
      "iteration 17700 / 20000: loss 1.850984\n",
      "iteration 17800 / 20000: loss 1.876564\n",
      "iteration 17900 / 20000: loss 1.936950\n",
      "iteration 18000 / 20000: loss 1.814896\n",
      "iteration 18100 / 20000: loss 1.846176\n",
      "iteration 18200 / 20000: loss 1.940851\n",
      "iteration 18300 / 20000: loss 1.906923\n",
      "iteration 18400 / 20000: loss 1.850915\n",
      "iteration 18500 / 20000: loss 1.949299\n",
      "iteration 18600 / 20000: loss 1.887061\n",
      "iteration 18700 / 20000: loss 1.861789\n",
      "iteration 18800 / 20000: loss 1.896102\n",
      "iteration 18900 / 20000: loss 1.906934\n",
      "iteration 19000 / 20000: loss 1.923183\n",
      "iteration 19100 / 20000: loss 1.884399\n",
      "iteration 19200 / 20000: loss 1.854194\n",
      "iteration 19300 / 20000: loss 1.839913\n",
      "iteration 19400 / 20000: loss 1.922940\n",
      "iteration 19500 / 20000: loss 1.938285\n",
      "iteration 19600 / 20000: loss 1.913621\n",
      "iteration 19700 / 20000: loss 1.855676\n",
      "iteration 19800 / 20000: loss 1.833512\n",
      "iteration 19900 / 20000: loss 1.862193\n",
      "lr, rs:  1e-07 50000.0\n",
      "iteration 0 / 20000: loss 778.478744\n",
      "iteration 100 / 20000: loss 285.688838\n",
      "iteration 200 / 20000: loss 105.894664\n",
      "iteration 300 / 20000: loss 39.976640\n",
      "iteration 400 / 20000: loss 15.957191\n",
      "iteration 500 / 20000: loss 7.197790\n",
      "iteration 600 / 20000: loss 3.998345\n",
      "iteration 700 / 20000: loss 2.810241\n",
      "iteration 800 / 20000: loss 2.339038\n",
      "iteration 900 / 20000: loss 2.175476\n",
      "iteration 1000 / 20000: loss 2.139453\n",
      "iteration 1100 / 20000: loss 2.128068\n",
      "iteration 1200 / 20000: loss 2.130851\n",
      "iteration 1300 / 20000: loss 2.063256\n",
      "iteration 1400 / 20000: loss 2.034551\n",
      "iteration 1500 / 20000: loss 2.081228\n",
      "iteration 1600 / 20000: loss 2.120414\n",
      "iteration 1700 / 20000: loss 2.096459\n",
      "iteration 1800 / 20000: loss 2.102069\n",
      "iteration 1900 / 20000: loss 2.106153\n",
      "iteration 2000 / 20000: loss 2.107060\n",
      "iteration 2100 / 20000: loss 2.099259\n",
      "iteration 2200 / 20000: loss 2.057436\n",
      "iteration 2300 / 20000: loss 2.027429\n",
      "iteration 2400 / 20000: loss 2.049217\n",
      "iteration 2500 / 20000: loss 2.081301\n",
      "iteration 2600 / 20000: loss 2.066216\n",
      "iteration 2700 / 20000: loss 2.068451\n",
      "iteration 2800 / 20000: loss 2.100045\n",
      "iteration 2900 / 20000: loss 2.085076\n",
      "iteration 3000 / 20000: loss 2.098802\n",
      "iteration 3100 / 20000: loss 2.072455\n",
      "iteration 3200 / 20000: loss 2.086592\n",
      "iteration 3300 / 20000: loss 2.070221\n",
      "iteration 3400 / 20000: loss 2.062251\n",
      "iteration 3500 / 20000: loss 2.115197\n",
      "iteration 3600 / 20000: loss 2.055412\n",
      "iteration 3700 / 20000: loss 2.134873\n",
      "iteration 3800 / 20000: loss 2.040613\n",
      "iteration 3900 / 20000: loss 2.058320\n",
      "iteration 4000 / 20000: loss 2.098183\n",
      "iteration 4100 / 20000: loss 2.093269\n",
      "iteration 4200 / 20000: loss 2.046329\n",
      "iteration 4300 / 20000: loss 2.093710\n",
      "iteration 4400 / 20000: loss 2.128026\n",
      "iteration 4500 / 20000: loss 2.045088\n",
      "iteration 4600 / 20000: loss 2.126645\n",
      "iteration 4700 / 20000: loss 2.096581\n",
      "iteration 4800 / 20000: loss 2.040733\n",
      "iteration 4900 / 20000: loss 2.068913\n",
      "iteration 5000 / 20000: loss 2.077190\n",
      "iteration 5100 / 20000: loss 2.066906\n",
      "iteration 5200 / 20000: loss 2.073526\n",
      "iteration 5300 / 20000: loss 2.143041\n",
      "iteration 5400 / 20000: loss 2.119345\n",
      "iteration 5500 / 20000: loss 2.084041\n",
      "iteration 5600 / 20000: loss 2.150719\n",
      "iteration 5700 / 20000: loss 2.090984\n",
      "iteration 5800 / 20000: loss 2.109310\n",
      "iteration 5900 / 20000: loss 2.119561\n",
      "iteration 6000 / 20000: loss 2.093309\n",
      "iteration 6100 / 20000: loss 2.080437\n",
      "iteration 6200 / 20000: loss 2.030742\n",
      "iteration 6300 / 20000: loss 2.145773\n",
      "iteration 6400 / 20000: loss 2.109525\n",
      "iteration 6500 / 20000: loss 2.052794\n",
      "iteration 6600 / 20000: loss 2.008976\n",
      "iteration 6700 / 20000: loss 2.107639\n",
      "iteration 6800 / 20000: loss 2.098068\n",
      "iteration 6900 / 20000: loss 2.069575\n",
      "iteration 7000 / 20000: loss 2.055588\n",
      "iteration 7100 / 20000: loss 2.100265\n",
      "iteration 7200 / 20000: loss 2.073083\n",
      "iteration 7300 / 20000: loss 2.109682\n",
      "iteration 7400 / 20000: loss 2.156780\n",
      "iteration 7500 / 20000: loss 2.044079\n",
      "iteration 7600 / 20000: loss 2.083482\n",
      "iteration 7700 / 20000: loss 2.087519\n",
      "iteration 7800 / 20000: loss 2.071623\n",
      "iteration 7900 / 20000: loss 2.092725\n",
      "iteration 8000 / 20000: loss 2.106207\n",
      "iteration 8100 / 20000: loss 2.083969\n",
      "iteration 8200 / 20000: loss 2.050038\n",
      "iteration 8300 / 20000: loss 2.081099\n",
      "iteration 8400 / 20000: loss 2.097473\n",
      "iteration 8500 / 20000: loss 2.082631\n",
      "iteration 8600 / 20000: loss 2.138003\n",
      "iteration 8700 / 20000: loss 2.073681\n",
      "iteration 8800 / 20000: loss 2.058186\n",
      "iteration 8900 / 20000: loss 2.091039\n",
      "iteration 9000 / 20000: loss 2.072679\n",
      "iteration 9100 / 20000: loss 2.107458\n",
      "iteration 9200 / 20000: loss 2.062933\n",
      "iteration 9300 / 20000: loss 2.104189\n",
      "iteration 9400 / 20000: loss 2.093119\n",
      "iteration 9500 / 20000: loss 2.129757\n",
      "iteration 9600 / 20000: loss 2.132900\n",
      "iteration 9700 / 20000: loss 2.093031\n",
      "iteration 9800 / 20000: loss 2.117167\n",
      "iteration 9900 / 20000: loss 2.077845\n",
      "iteration 10000 / 20000: loss 2.112882\n",
      "iteration 10100 / 20000: loss 2.103462\n",
      "iteration 10200 / 20000: loss 2.102565\n",
      "iteration 10300 / 20000: loss 2.054388\n",
      "iteration 10400 / 20000: loss 2.112721\n",
      "iteration 10500 / 20000: loss 2.112568\n",
      "iteration 10600 / 20000: loss 2.087169\n",
      "iteration 10700 / 20000: loss 2.076325\n",
      "iteration 10800 / 20000: loss 2.092476\n",
      "iteration 10900 / 20000: loss 2.081817\n",
      "iteration 11000 / 20000: loss 2.071743\n",
      "iteration 11100 / 20000: loss 2.098297\n",
      "iteration 11200 / 20000: loss 2.033128\n",
      "iteration 11300 / 20000: loss 2.104237\n",
      "iteration 11400 / 20000: loss 2.082451\n",
      "iteration 11500 / 20000: loss 2.071157\n",
      "iteration 11600 / 20000: loss 2.099816\n",
      "iteration 11700 / 20000: loss 2.090412\n",
      "iteration 11800 / 20000: loss 2.106050\n",
      "iteration 11900 / 20000: loss 2.035886\n",
      "iteration 12000 / 20000: loss 2.055470\n",
      "iteration 12100 / 20000: loss 2.124692\n",
      "iteration 12200 / 20000: loss 2.076160\n",
      "iteration 12300 / 20000: loss 2.134793\n",
      "iteration 12400 / 20000: loss 2.083528\n",
      "iteration 12500 / 20000: loss 2.045269\n",
      "iteration 12600 / 20000: loss 2.062874\n",
      "iteration 12700 / 20000: loss 2.083476\n",
      "iteration 12800 / 20000: loss 2.090796\n",
      "iteration 12900 / 20000: loss 2.046788\n",
      "iteration 13000 / 20000: loss 2.039863\n",
      "iteration 13100 / 20000: loss 2.087054\n",
      "iteration 13200 / 20000: loss 2.082453\n",
      "iteration 13300 / 20000: loss 2.075806\n",
      "iteration 13400 / 20000: loss 2.135364\n",
      "iteration 13500 / 20000: loss 2.107487\n",
      "iteration 13600 / 20000: loss 2.099558\n",
      "iteration 13700 / 20000: loss 2.090976\n",
      "iteration 13800 / 20000: loss 2.085225\n",
      "iteration 13900 / 20000: loss 2.110592\n",
      "iteration 14000 / 20000: loss 2.129520\n",
      "iteration 14100 / 20000: loss 2.056849\n",
      "iteration 14200 / 20000: loss 2.090763\n",
      "iteration 14300 / 20000: loss 2.016541\n",
      "iteration 14400 / 20000: loss 2.081252\n",
      "iteration 14500 / 20000: loss 2.098348\n",
      "iteration 14600 / 20000: loss 2.012625\n",
      "iteration 14700 / 20000: loss 2.118323\n",
      "iteration 14800 / 20000: loss 2.156315\n",
      "iteration 14900 / 20000: loss 2.094214\n",
      "iteration 15000 / 20000: loss 2.066899\n",
      "iteration 15100 / 20000: loss 2.107017\n",
      "iteration 15200 / 20000: loss 2.063996\n",
      "iteration 15300 / 20000: loss 2.084738\n",
      "iteration 15400 / 20000: loss 2.115539\n",
      "iteration 15500 / 20000: loss 2.051211\n",
      "iteration 15600 / 20000: loss 2.074851\n",
      "iteration 15700 / 20000: loss 2.091771\n",
      "iteration 15800 / 20000: loss 2.118252\n",
      "iteration 15900 / 20000: loss 2.175888\n",
      "iteration 16000 / 20000: loss 2.019614\n",
      "iteration 16100 / 20000: loss 2.144186\n",
      "iteration 16200 / 20000: loss 2.080917\n",
      "iteration 16300 / 20000: loss 2.092924\n",
      "iteration 16400 / 20000: loss 2.108603\n",
      "iteration 16500 / 20000: loss 2.090302\n",
      "iteration 16600 / 20000: loss 2.113304\n",
      "iteration 16700 / 20000: loss 2.103117\n",
      "iteration 16800 / 20000: loss 2.086128\n",
      "iteration 16900 / 20000: loss 2.047656\n",
      "iteration 17000 / 20000: loss 2.116529\n",
      "iteration 17100 / 20000: loss 2.061010\n",
      "iteration 17200 / 20000: loss 2.043928\n",
      "iteration 17300 / 20000: loss 2.109570\n",
      "iteration 17400 / 20000: loss 2.140278\n",
      "iteration 17500 / 20000: loss 2.069552\n",
      "iteration 17600 / 20000: loss 2.090708\n",
      "iteration 17700 / 20000: loss 2.047139\n",
      "iteration 17800 / 20000: loss 2.094352\n",
      "iteration 17900 / 20000: loss 2.092056\n",
      "iteration 18000 / 20000: loss 2.062740\n",
      "iteration 18100 / 20000: loss 2.015396\n",
      "iteration 18200 / 20000: loss 2.116178\n",
      "iteration 18300 / 20000: loss 2.107590\n",
      "iteration 18400 / 20000: loss 2.129058\n",
      "iteration 18500 / 20000: loss 2.116379\n",
      "iteration 18600 / 20000: loss 2.049263\n",
      "iteration 18700 / 20000: loss 2.123430\n",
      "iteration 18800 / 20000: loss 2.066838\n",
      "iteration 18900 / 20000: loss 2.098226\n",
      "iteration 19000 / 20000: loss 2.023504\n",
      "iteration 19100 / 20000: loss 2.108218\n",
      "iteration 19200 / 20000: loss 2.041716\n",
      "iteration 19300 / 20000: loss 2.050490\n",
      "iteration 19400 / 20000: loss 2.097079\n",
      "iteration 19500 / 20000: loss 2.024916\n",
      "iteration 19600 / 20000: loss 2.124162\n",
      "iteration 19700 / 20000: loss 2.051713\n",
      "iteration 19800 / 20000: loss 2.138058\n",
      "iteration 19900 / 20000: loss 2.130633\n",
      "lr, rs:  5e-07 1000.0\n",
      "iteration 0 / 20000: loss 20.852246\n",
      "iteration 100 / 20000: loss 16.873001\n",
      "iteration 200 / 20000: loss 14.964964\n",
      "iteration 300 / 20000: loss 13.472140\n",
      "iteration 400 / 20000: loss 12.356246\n",
      "iteration 500 / 20000: loss 11.114717\n",
      "iteration 600 / 20000: loss 10.292237\n",
      "iteration 700 / 20000: loss 9.597364\n",
      "iteration 800 / 20000: loss 8.590692\n",
      "iteration 900 / 20000: loss 7.965487\n",
      "iteration 1000 / 20000: loss 7.284475\n",
      "iteration 1100 / 20000: loss 6.743075\n",
      "iteration 1200 / 20000: loss 6.279225\n",
      "iteration 1300 / 20000: loss 5.892493\n",
      "iteration 1400 / 20000: loss 5.466275\n",
      "iteration 1500 / 20000: loss 5.055572\n",
      "iteration 1600 / 20000: loss 4.692156\n",
      "iteration 1700 / 20000: loss 4.541561\n",
      "iteration 1800 / 20000: loss 4.269829\n",
      "iteration 1900 / 20000: loss 3.930734\n",
      "iteration 2000 / 20000: loss 3.827837\n",
      "iteration 2100 / 20000: loss 3.585344\n",
      "iteration 2200 / 20000: loss 3.385204\n",
      "iteration 2300 / 20000: loss 3.251076\n",
      "iteration 2400 / 20000: loss 3.105204\n",
      "iteration 2500 / 20000: loss 2.941190\n",
      "iteration 2600 / 20000: loss 2.848117\n",
      "iteration 2700 / 20000: loss 2.687644\n",
      "iteration 2800 / 20000: loss 2.737282\n",
      "iteration 2900 / 20000: loss 2.544581\n",
      "iteration 3000 / 20000: loss 2.418296\n",
      "iteration 3100 / 20000: loss 2.501832\n",
      "iteration 3200 / 20000: loss 2.376561\n",
      "iteration 3300 / 20000: loss 2.397374\n",
      "iteration 3400 / 20000: loss 2.218378\n",
      "iteration 3500 / 20000: loss 2.231551\n",
      "iteration 3600 / 20000: loss 2.185918\n",
      "iteration 3700 / 20000: loss 2.148834\n",
      "iteration 3800 / 20000: loss 2.105862\n",
      "iteration 3900 / 20000: loss 1.978260\n",
      "iteration 4000 / 20000: loss 2.149178\n",
      "iteration 4100 / 20000: loss 2.046081\n",
      "iteration 4200 / 20000: loss 1.946870\n",
      "iteration 4300 / 20000: loss 2.082118\n",
      "iteration 4400 / 20000: loss 1.896961\n",
      "iteration 4500 / 20000: loss 2.046241\n",
      "iteration 4600 / 20000: loss 1.950572\n",
      "iteration 4700 / 20000: loss 1.910063\n",
      "iteration 4800 / 20000: loss 1.913474\n",
      "iteration 4900 / 20000: loss 1.994126\n",
      "iteration 5000 / 20000: loss 1.854145\n",
      "iteration 5100 / 20000: loss 1.821439\n",
      "iteration 5200 / 20000: loss 1.820662\n",
      "iteration 5300 / 20000: loss 1.854938\n",
      "iteration 5400 / 20000: loss 1.829091\n",
      "iteration 5500 / 20000: loss 1.850253\n",
      "iteration 5600 / 20000: loss 1.833653\n",
      "iteration 5700 / 20000: loss 1.862035\n",
      "iteration 5800 / 20000: loss 1.841974\n",
      "iteration 5900 / 20000: loss 1.784179\n",
      "iteration 6000 / 20000: loss 1.804002\n",
      "iteration 6100 / 20000: loss 1.793918\n",
      "iteration 6200 / 20000: loss 1.871272\n",
      "iteration 6300 / 20000: loss 1.874920\n",
      "iteration 6400 / 20000: loss 1.773964\n",
      "iteration 6500 / 20000: loss 1.816792\n",
      "iteration 6600 / 20000: loss 1.769152\n",
      "iteration 6700 / 20000: loss 1.721165\n",
      "iteration 6800 / 20000: loss 1.889130\n",
      "iteration 6900 / 20000: loss 1.694072\n",
      "iteration 7000 / 20000: loss 1.691273\n",
      "iteration 7100 / 20000: loss 1.938514\n",
      "iteration 7200 / 20000: loss 1.778871\n",
      "iteration 7300 / 20000: loss 1.824945\n",
      "iteration 7400 / 20000: loss 1.709757\n",
      "iteration 7500 / 20000: loss 1.852850\n",
      "iteration 7600 / 20000: loss 1.827068\n",
      "iteration 7700 / 20000: loss 1.751063\n",
      "iteration 7800 / 20000: loss 1.811745\n",
      "iteration 7900 / 20000: loss 1.782975\n",
      "iteration 8000 / 20000: loss 1.705386\n",
      "iteration 8100 / 20000: loss 1.868801\n",
      "iteration 8200 / 20000: loss 1.773495\n",
      "iteration 8300 / 20000: loss 1.780913\n",
      "iteration 8400 / 20000: loss 1.748422\n",
      "iteration 8500 / 20000: loss 1.796298\n",
      "iteration 8600 / 20000: loss 1.764989\n",
      "iteration 8700 / 20000: loss 1.669102\n",
      "iteration 8800 / 20000: loss 1.694932\n",
      "iteration 8900 / 20000: loss 1.759762\n",
      "iteration 9000 / 20000: loss 1.845493\n",
      "iteration 9100 / 20000: loss 1.814308\n",
      "iteration 9200 / 20000: loss 1.741189\n",
      "iteration 9300 / 20000: loss 1.722963\n",
      "iteration 9400 / 20000: loss 1.766902\n",
      "iteration 9500 / 20000: loss 1.699546\n",
      "iteration 9600 / 20000: loss 1.846431\n",
      "iteration 9700 / 20000: loss 1.813390\n",
      "iteration 9800 / 20000: loss 1.730430\n",
      "iteration 9900 / 20000: loss 1.735465\n",
      "iteration 10000 / 20000: loss 1.762540\n",
      "iteration 10100 / 20000: loss 1.769815\n",
      "iteration 10200 / 20000: loss 1.691163\n",
      "iteration 10300 / 20000: loss 1.672134\n",
      "iteration 10400 / 20000: loss 1.809876\n",
      "iteration 10500 / 20000: loss 1.845124\n",
      "iteration 10600 / 20000: loss 1.730328\n",
      "iteration 10700 / 20000: loss 1.758262\n",
      "iteration 10800 / 20000: loss 1.815656\n",
      "iteration 10900 / 20000: loss 1.720949\n",
      "iteration 11000 / 20000: loss 1.759190\n",
      "iteration 11100 / 20000: loss 1.751197\n",
      "iteration 11200 / 20000: loss 1.658648\n",
      "iteration 11300 / 20000: loss 1.725224\n",
      "iteration 11400 / 20000: loss 1.737599\n",
      "iteration 11500 / 20000: loss 1.917162\n",
      "iteration 11600 / 20000: loss 1.731313\n",
      "iteration 11700 / 20000: loss 1.746396\n",
      "iteration 11800 / 20000: loss 1.698292\n",
      "iteration 11900 / 20000: loss 1.788232\n",
      "iteration 12000 / 20000: loss 1.773840\n",
      "iteration 12100 / 20000: loss 1.804018\n",
      "iteration 12200 / 20000: loss 1.643887\n",
      "iteration 12300 / 20000: loss 1.753055\n",
      "iteration 12400 / 20000: loss 1.761833\n",
      "iteration 12500 / 20000: loss 1.818099\n",
      "iteration 12600 / 20000: loss 1.803156\n",
      "iteration 12700 / 20000: loss 1.860073\n",
      "iteration 12800 / 20000: loss 1.694758\n",
      "iteration 12900 / 20000: loss 1.867268\n",
      "iteration 13000 / 20000: loss 1.761778\n",
      "iteration 13100 / 20000: loss 1.743172\n",
      "iteration 13200 / 20000: loss 1.804391\n",
      "iteration 13300 / 20000: loss 1.695768\n",
      "iteration 13400 / 20000: loss 1.694377\n",
      "iteration 13500 / 20000: loss 1.758983\n",
      "iteration 13600 / 20000: loss 1.851506\n",
      "iteration 13700 / 20000: loss 1.791469\n",
      "iteration 13800 / 20000: loss 1.762325\n",
      "iteration 13900 / 20000: loss 1.810141\n",
      "iteration 14000 / 20000: loss 1.734439\n",
      "iteration 14100 / 20000: loss 1.750802\n",
      "iteration 14200 / 20000: loss 1.850846\n",
      "iteration 14300 / 20000: loss 1.737967\n",
      "iteration 14400 / 20000: loss 1.897509\n",
      "iteration 14500 / 20000: loss 1.762042\n",
      "iteration 14600 / 20000: loss 1.801603\n",
      "iteration 14700 / 20000: loss 1.776319\n",
      "iteration 14800 / 20000: loss 1.851695\n",
      "iteration 14900 / 20000: loss 1.727865\n",
      "iteration 15000 / 20000: loss 1.813171\n",
      "iteration 15100 / 20000: loss 1.761682\n",
      "iteration 15200 / 20000: loss 1.792137\n",
      "iteration 15300 / 20000: loss 1.855508\n",
      "iteration 15400 / 20000: loss 1.722691\n",
      "iteration 15500 / 20000: loss 1.797364\n",
      "iteration 15600 / 20000: loss 1.708479\n",
      "iteration 15700 / 20000: loss 1.794097\n",
      "iteration 15800 / 20000: loss 1.762621\n",
      "iteration 15900 / 20000: loss 1.608766\n",
      "iteration 16000 / 20000: loss 1.719700\n",
      "iteration 16100 / 20000: loss 1.823477\n",
      "iteration 16200 / 20000: loss 1.863429\n",
      "iteration 16300 / 20000: loss 1.780752\n",
      "iteration 16400 / 20000: loss 1.721732\n",
      "iteration 16500 / 20000: loss 1.753807\n",
      "iteration 16600 / 20000: loss 1.895749\n",
      "iteration 16700 / 20000: loss 1.756553\n",
      "iteration 16800 / 20000: loss 1.739035\n",
      "iteration 16900 / 20000: loss 1.819179\n",
      "iteration 17000 / 20000: loss 1.752171\n",
      "iteration 17100 / 20000: loss 1.701150\n",
      "iteration 17200 / 20000: loss 1.701189\n",
      "iteration 17300 / 20000: loss 1.670151\n",
      "iteration 17400 / 20000: loss 1.826198\n",
      "iteration 17500 / 20000: loss 1.899597\n",
      "iteration 17600 / 20000: loss 1.909027\n",
      "iteration 17700 / 20000: loss 1.839000\n",
      "iteration 17800 / 20000: loss 1.707300\n",
      "iteration 17900 / 20000: loss 1.751698\n",
      "iteration 18000 / 20000: loss 1.581284\n",
      "iteration 18100 / 20000: loss 1.782856\n",
      "iteration 18200 / 20000: loss 1.849319\n",
      "iteration 18300 / 20000: loss 1.898253\n",
      "iteration 18400 / 20000: loss 1.785536\n",
      "iteration 18500 / 20000: loss 1.816773\n",
      "iteration 18600 / 20000: loss 1.855210\n",
      "iteration 18700 / 20000: loss 1.811595\n",
      "iteration 18800 / 20000: loss 1.854512\n",
      "iteration 18900 / 20000: loss 1.737978\n",
      "iteration 19000 / 20000: loss 1.743928\n",
      "iteration 19100 / 20000: loss 1.816262\n",
      "iteration 19200 / 20000: loss 1.831334\n",
      "iteration 19300 / 20000: loss 1.694800\n",
      "iteration 19400 / 20000: loss 1.809862\n",
      "iteration 19500 / 20000: loss 1.720030\n",
      "iteration 19600 / 20000: loss 1.770464\n",
      "iteration 19700 / 20000: loss 1.814064\n",
      "iteration 19800 / 20000: loss 1.758651\n",
      "iteration 19900 / 20000: loss 1.728864\n",
      "lr, rs:  5e-07 5000.0\n",
      "iteration 0 / 20000: loss 81.180925\n",
      "iteration 100 / 20000: loss 48.816668\n",
      "iteration 200 / 20000: loss 30.005006\n",
      "iteration 300 / 20000: loss 18.832534\n",
      "iteration 400 / 20000: loss 12.075204\n",
      "iteration 500 / 20000: loss 8.037594\n",
      "iteration 600 / 20000: loss 5.559618\n",
      "iteration 700 / 20000: loss 4.119760\n",
      "iteration 800 / 20000: loss 3.303779\n",
      "iteration 900 / 20000: loss 2.684801\n",
      "iteration 1000 / 20000: loss 2.348816\n",
      "iteration 1100 / 20000: loss 2.206231\n",
      "iteration 1200 / 20000: loss 2.073630\n",
      "iteration 1300 / 20000: loss 1.873317\n",
      "iteration 1400 / 20000: loss 1.941253\n",
      "iteration 1500 / 20000: loss 1.951903\n",
      "iteration 1600 / 20000: loss 1.911394\n",
      "iteration 1700 / 20000: loss 1.894354\n",
      "iteration 1800 / 20000: loss 1.826329\n",
      "iteration 1900 / 20000: loss 1.911520\n",
      "iteration 2000 / 20000: loss 1.887104\n",
      "iteration 2100 / 20000: loss 1.806221\n",
      "iteration 2200 / 20000: loss 1.932529\n",
      "iteration 2300 / 20000: loss 1.906828\n",
      "iteration 2400 / 20000: loss 1.981768\n",
      "iteration 2500 / 20000: loss 1.884023\n",
      "iteration 2600 / 20000: loss 1.854541\n",
      "iteration 2700 / 20000: loss 1.815310\n",
      "iteration 2800 / 20000: loss 1.839291\n",
      "iteration 2900 / 20000: loss 1.862224\n",
      "iteration 3000 / 20000: loss 1.892659\n",
      "iteration 3100 / 20000: loss 1.949833\n",
      "iteration 3200 / 20000: loss 1.944275\n",
      "iteration 3300 / 20000: loss 1.984230\n",
      "iteration 3400 / 20000: loss 1.888343\n",
      "iteration 3500 / 20000: loss 1.847059\n",
      "iteration 3600 / 20000: loss 1.890856\n",
      "iteration 3700 / 20000: loss 1.965377\n",
      "iteration 3800 / 20000: loss 1.889958\n",
      "iteration 3900 / 20000: loss 1.955631\n",
      "iteration 4000 / 20000: loss 1.865437\n",
      "iteration 4100 / 20000: loss 1.849239\n",
      "iteration 4200 / 20000: loss 1.971826\n",
      "iteration 4300 / 20000: loss 1.900146\n",
      "iteration 4400 / 20000: loss 1.860102\n",
      "iteration 4500 / 20000: loss 1.880034\n",
      "iteration 4600 / 20000: loss 1.870923\n",
      "iteration 4700 / 20000: loss 1.765909\n",
      "iteration 4800 / 20000: loss 1.939592\n",
      "iteration 4900 / 20000: loss 1.977290\n",
      "iteration 5000 / 20000: loss 1.940135\n",
      "iteration 5100 / 20000: loss 1.928648\n",
      "iteration 5200 / 20000: loss 1.862077\n",
      "iteration 5300 / 20000: loss 1.926456\n",
      "iteration 5400 / 20000: loss 1.920682\n",
      "iteration 5500 / 20000: loss 1.926035\n",
      "iteration 5600 / 20000: loss 1.905321\n",
      "iteration 5700 / 20000: loss 1.871864\n",
      "iteration 5800 / 20000: loss 1.880607\n",
      "iteration 5900 / 20000: loss 1.781193\n",
      "iteration 6000 / 20000: loss 1.976015\n",
      "iteration 6100 / 20000: loss 1.955707\n",
      "iteration 6200 / 20000: loss 1.887145\n",
      "iteration 6300 / 20000: loss 1.835325\n",
      "iteration 6400 / 20000: loss 1.932510\n",
      "iteration 6500 / 20000: loss 1.857393\n",
      "iteration 6600 / 20000: loss 1.892213\n",
      "iteration 6700 / 20000: loss 1.889425\n",
      "iteration 6800 / 20000: loss 1.845359\n",
      "iteration 6900 / 20000: loss 2.017133\n",
      "iteration 7000 / 20000: loss 1.933406\n",
      "iteration 7100 / 20000: loss 1.952936\n",
      "iteration 7200 / 20000: loss 1.867677\n",
      "iteration 7300 / 20000: loss 1.845636\n",
      "iteration 7400 / 20000: loss 1.906364\n",
      "iteration 7500 / 20000: loss 1.955130\n",
      "iteration 7600 / 20000: loss 1.915127\n",
      "iteration 7700 / 20000: loss 1.868905\n",
      "iteration 7800 / 20000: loss 1.855545\n",
      "iteration 7900 / 20000: loss 1.871893\n",
      "iteration 8000 / 20000: loss 1.940854\n",
      "iteration 8100 / 20000: loss 1.882635\n",
      "iteration 8200 / 20000: loss 1.855722\n",
      "iteration 8300 / 20000: loss 1.824610\n",
      "iteration 8400 / 20000: loss 1.960340\n",
      "iteration 8500 / 20000: loss 1.870584\n",
      "iteration 8600 / 20000: loss 2.002889\n",
      "iteration 8700 / 20000: loss 1.793684\n",
      "iteration 8800 / 20000: loss 1.896580\n",
      "iteration 8900 / 20000: loss 1.941924\n",
      "iteration 9000 / 20000: loss 1.938028\n",
      "iteration 9100 / 20000: loss 1.957629\n",
      "iteration 9200 / 20000: loss 1.760397\n",
      "iteration 9300 / 20000: loss 1.849966\n",
      "iteration 9400 / 20000: loss 1.848544\n",
      "iteration 9500 / 20000: loss 1.933805\n",
      "iteration 9600 / 20000: loss 1.827688\n",
      "iteration 9700 / 20000: loss 1.909884\n",
      "iteration 9800 / 20000: loss 1.881922\n",
      "iteration 9900 / 20000: loss 1.871562\n",
      "iteration 10000 / 20000: loss 1.918354\n",
      "iteration 10100 / 20000: loss 1.907784\n",
      "iteration 10200 / 20000: loss 1.861255\n",
      "iteration 10300 / 20000: loss 1.833135\n",
      "iteration 10400 / 20000: loss 2.032555\n",
      "iteration 10500 / 20000: loss 1.841477\n",
      "iteration 10600 / 20000: loss 1.889270\n",
      "iteration 10700 / 20000: loss 1.967415\n",
      "iteration 10800 / 20000: loss 1.791455\n",
      "iteration 10900 / 20000: loss 1.928792\n",
      "iteration 11000 / 20000: loss 1.915164\n",
      "iteration 11100 / 20000: loss 1.936619\n",
      "iteration 11200 / 20000: loss 1.806319\n",
      "iteration 11300 / 20000: loss 1.867157\n",
      "iteration 11400 / 20000: loss 1.925154\n",
      "iteration 11500 / 20000: loss 1.848205\n",
      "iteration 11600 / 20000: loss 1.942044\n",
      "iteration 11700 / 20000: loss 1.738722\n",
      "iteration 11800 / 20000: loss 1.743696\n",
      "iteration 11900 / 20000: loss 1.882667\n",
      "iteration 12000 / 20000: loss 1.976444\n",
      "iteration 12100 / 20000: loss 1.930452\n",
      "iteration 12200 / 20000: loss 1.884696\n",
      "iteration 12300 / 20000: loss 1.844938\n",
      "iteration 12400 / 20000: loss 1.951229\n",
      "iteration 12500 / 20000: loss 1.884545\n",
      "iteration 12600 / 20000: loss 1.928600\n",
      "iteration 12700 / 20000: loss 1.859065\n",
      "iteration 12800 / 20000: loss 1.930546\n",
      "iteration 12900 / 20000: loss 1.831186\n",
      "iteration 13000 / 20000: loss 1.922074\n",
      "iteration 13100 / 20000: loss 1.842376\n",
      "iteration 13200 / 20000: loss 1.981325\n",
      "iteration 13300 / 20000: loss 1.876216\n",
      "iteration 13400 / 20000: loss 1.924341\n",
      "iteration 13500 / 20000: loss 1.975751\n",
      "iteration 13600 / 20000: loss 1.992319\n",
      "iteration 13700 / 20000: loss 1.889908\n",
      "iteration 13800 / 20000: loss 1.837006\n",
      "iteration 13900 / 20000: loss 1.920670\n",
      "iteration 14000 / 20000: loss 1.913229\n",
      "iteration 14100 / 20000: loss 1.838588\n",
      "iteration 14200 / 20000: loss 1.810701\n",
      "iteration 14300 / 20000: loss 1.879830\n",
      "iteration 14400 / 20000: loss 1.951314\n",
      "iteration 14500 / 20000: loss 1.932435\n",
      "iteration 14600 / 20000: loss 1.933971\n",
      "iteration 14700 / 20000: loss 1.926747\n",
      "iteration 14800 / 20000: loss 1.917104\n",
      "iteration 14900 / 20000: loss 1.912512\n",
      "iteration 15000 / 20000: loss 1.903131\n",
      "iteration 15100 / 20000: loss 1.917063\n",
      "iteration 15200 / 20000: loss 1.869996\n",
      "iteration 15300 / 20000: loss 1.946734\n",
      "iteration 15400 / 20000: loss 1.871904\n",
      "iteration 15500 / 20000: loss 1.857587\n",
      "iteration 15600 / 20000: loss 1.820281\n",
      "iteration 15700 / 20000: loss 1.895560\n",
      "iteration 15800 / 20000: loss 1.921499\n",
      "iteration 15900 / 20000: loss 1.892077\n",
      "iteration 16000 / 20000: loss 1.814758\n",
      "iteration 16100 / 20000: loss 1.916670\n",
      "iteration 16200 / 20000: loss 1.937446\n",
      "iteration 16300 / 20000: loss 1.917508\n",
      "iteration 16400 / 20000: loss 1.919725\n",
      "iteration 16500 / 20000: loss 1.912136\n",
      "iteration 16600 / 20000: loss 1.800258\n",
      "iteration 16700 / 20000: loss 1.921895\n",
      "iteration 16800 / 20000: loss 2.004539\n",
      "iteration 16900 / 20000: loss 1.983281\n",
      "iteration 17000 / 20000: loss 1.971975\n",
      "iteration 17100 / 20000: loss 1.943825\n",
      "iteration 17200 / 20000: loss 1.865873\n",
      "iteration 17300 / 20000: loss 1.923484\n",
      "iteration 17400 / 20000: loss 1.853237\n",
      "iteration 17500 / 20000: loss 1.811164\n",
      "iteration 17600 / 20000: loss 1.958799\n",
      "iteration 17700 / 20000: loss 1.869234\n",
      "iteration 17800 / 20000: loss 1.904083\n",
      "iteration 17900 / 20000: loss 2.016754\n",
      "iteration 18000 / 20000: loss 1.841088\n",
      "iteration 18100 / 20000: loss 1.895412\n",
      "iteration 18200 / 20000: loss 1.850592\n",
      "iteration 18300 / 20000: loss 1.892919\n",
      "iteration 18400 / 20000: loss 1.895878\n",
      "iteration 18500 / 20000: loss 1.941269\n",
      "iteration 18600 / 20000: loss 1.809146\n",
      "iteration 18700 / 20000: loss 1.928107\n",
      "iteration 18800 / 20000: loss 1.994412\n",
      "iteration 18900 / 20000: loss 1.918074\n",
      "iteration 19000 / 20000: loss 1.889793\n",
      "iteration 19100 / 20000: loss 1.938033\n",
      "iteration 19200 / 20000: loss 1.958518\n",
      "iteration 19300 / 20000: loss 1.931701\n",
      "iteration 19400 / 20000: loss 1.907462\n",
      "iteration 19500 / 20000: loss 1.837911\n",
      "iteration 19600 / 20000: loss 2.068871\n",
      "iteration 19700 / 20000: loss 1.951062\n",
      "iteration 19800 / 20000: loss 1.826024\n",
      "iteration 19900 / 20000: loss 1.900858\n",
      "lr, rs:  5e-07 50000.0\n",
      "iteration 0 / 20000: loss 780.778631\n",
      "iteration 100 / 20000: loss 6.927169\n",
      "iteration 200 / 20000: loss 2.102056\n",
      "iteration 300 / 20000: loss 2.172594\n",
      "iteration 400 / 20000: loss 2.066578\n",
      "iteration 500 / 20000: loss 2.073174\n",
      "iteration 600 / 20000: loss 2.095809\n",
      "iteration 700 / 20000: loss 2.083214\n",
      "iteration 800 / 20000: loss 2.080570\n",
      "iteration 900 / 20000: loss 2.085709\n",
      "iteration 1000 / 20000: loss 2.042270\n",
      "iteration 1100 / 20000: loss 2.107587\n",
      "iteration 1200 / 20000: loss 2.112059\n",
      "iteration 1300 / 20000: loss 2.120489\n",
      "iteration 1400 / 20000: loss 2.066121\n",
      "iteration 1500 / 20000: loss 2.025616\n",
      "iteration 1600 / 20000: loss 2.076916\n",
      "iteration 1700 / 20000: loss 2.077713\n",
      "iteration 1800 / 20000: loss 2.080670\n",
      "iteration 1900 / 20000: loss 2.141119\n",
      "iteration 2000 / 20000: loss 2.056108\n",
      "iteration 2100 / 20000: loss 2.046056\n",
      "iteration 2200 / 20000: loss 2.095092\n",
      "iteration 2300 / 20000: loss 2.052139\n",
      "iteration 2400 / 20000: loss 2.128038\n",
      "iteration 2500 / 20000: loss 2.078546\n",
      "iteration 2600 / 20000: loss 2.125067\n",
      "iteration 2700 / 20000: loss 2.053712\n",
      "iteration 2800 / 20000: loss 2.084031\n",
      "iteration 2900 / 20000: loss 2.090634\n",
      "iteration 3000 / 20000: loss 2.030765\n",
      "iteration 3100 / 20000: loss 2.032973\n",
      "iteration 3200 / 20000: loss 2.065579\n",
      "iteration 3300 / 20000: loss 2.071510\n",
      "iteration 3400 / 20000: loss 2.087554\n",
      "iteration 3500 / 20000: loss 2.093020\n",
      "iteration 3600 / 20000: loss 2.114042\n",
      "iteration 3700 / 20000: loss 2.149427\n",
      "iteration 3800 / 20000: loss 2.129244\n",
      "iteration 3900 / 20000: loss 2.087211\n",
      "iteration 4000 / 20000: loss 2.082058\n",
      "iteration 4100 / 20000: loss 2.062313\n",
      "iteration 4200 / 20000: loss 2.037804\n",
      "iteration 4300 / 20000: loss 2.125525\n",
      "iteration 4400 / 20000: loss 2.075488\n",
      "iteration 4500 / 20000: loss 2.046918\n",
      "iteration 4600 / 20000: loss 2.104666\n",
      "iteration 4700 / 20000: loss 2.073335\n",
      "iteration 4800 / 20000: loss 2.104118\n",
      "iteration 4900 / 20000: loss 2.129944\n",
      "iteration 5000 / 20000: loss 2.103962\n",
      "iteration 5100 / 20000: loss 2.086930\n",
      "iteration 5200 / 20000: loss 2.092356\n",
      "iteration 5300 / 20000: loss 2.095991\n",
      "iteration 5400 / 20000: loss 2.145482\n",
      "iteration 5500 / 20000: loss 2.031368\n",
      "iteration 5600 / 20000: loss 2.117506\n",
      "iteration 5700 / 20000: loss 2.042456\n",
      "iteration 5800 / 20000: loss 2.074778\n",
      "iteration 5900 / 20000: loss 2.068918\n",
      "iteration 6000 / 20000: loss 2.124002\n",
      "iteration 6100 / 20000: loss 2.146427\n",
      "iteration 6200 / 20000: loss 2.089978\n",
      "iteration 6300 / 20000: loss 2.089112\n",
      "iteration 6400 / 20000: loss 2.137433\n",
      "iteration 6500 / 20000: loss 2.088275\n",
      "iteration 6600 / 20000: loss 2.049480\n",
      "iteration 6700 / 20000: loss 2.061298\n",
      "iteration 6800 / 20000: loss 2.006655\n",
      "iteration 6900 / 20000: loss 2.065024\n",
      "iteration 7000 / 20000: loss 2.053203\n",
      "iteration 7100 / 20000: loss 2.112659\n",
      "iteration 7200 / 20000: loss 2.093619\n",
      "iteration 7300 / 20000: loss 2.093012\n",
      "iteration 7400 / 20000: loss 2.042476\n",
      "iteration 7500 / 20000: loss 2.100805\n",
      "iteration 7600 / 20000: loss 2.070558\n",
      "iteration 7700 / 20000: loss 2.074876\n",
      "iteration 7800 / 20000: loss 2.093242\n",
      "iteration 7900 / 20000: loss 2.096315\n",
      "iteration 8000 / 20000: loss 2.109398\n",
      "iteration 8100 / 20000: loss 2.105371\n",
      "iteration 8200 / 20000: loss 2.052275\n",
      "iteration 8300 / 20000: loss 2.082696\n",
      "iteration 8400 / 20000: loss 2.114133\n",
      "iteration 8500 / 20000: loss 2.133206\n",
      "iteration 8600 / 20000: loss 2.102992\n",
      "iteration 8700 / 20000: loss 2.079773\n",
      "iteration 8800 / 20000: loss 2.108536\n",
      "iteration 8900 / 20000: loss 2.015194\n",
      "iteration 9000 / 20000: loss 2.043877\n",
      "iteration 9100 / 20000: loss 2.122888\n",
      "iteration 9200 / 20000: loss 2.104534\n",
      "iteration 9300 / 20000: loss 2.091215\n",
      "iteration 9400 / 20000: loss 2.075384\n",
      "iteration 9500 / 20000: loss 2.177796\n",
      "iteration 9600 / 20000: loss 2.131197\n",
      "iteration 9700 / 20000: loss 2.119264\n",
      "iteration 9800 / 20000: loss 2.084156\n",
      "iteration 9900 / 20000: loss 2.095884\n",
      "iteration 10000 / 20000: loss 2.153515\n",
      "iteration 10100 / 20000: loss 2.169630\n",
      "iteration 10200 / 20000: loss 2.125846\n",
      "iteration 10300 / 20000: loss 2.086344\n",
      "iteration 10400 / 20000: loss 2.092817\n",
      "iteration 10500 / 20000: loss 2.044909\n",
      "iteration 10600 / 20000: loss 2.052099\n",
      "iteration 10700 / 20000: loss 2.099923\n",
      "iteration 10800 / 20000: loss 2.088562\n",
      "iteration 10900 / 20000: loss 2.063970\n",
      "iteration 11000 / 20000: loss 2.056962\n",
      "iteration 11100 / 20000: loss 2.100041\n",
      "iteration 11200 / 20000: loss 2.113094\n",
      "iteration 11300 / 20000: loss 2.108444\n",
      "iteration 11400 / 20000: loss 2.127573\n",
      "iteration 11500 / 20000: loss 2.074118\n",
      "iteration 11600 / 20000: loss 2.046458\n",
      "iteration 11700 / 20000: loss 2.095364\n",
      "iteration 11800 / 20000: loss 2.079759\n",
      "iteration 11900 / 20000: loss 2.090903\n",
      "iteration 12000 / 20000: loss 2.097402\n",
      "iteration 12100 / 20000: loss 2.135208\n",
      "iteration 12200 / 20000: loss 2.110593\n",
      "iteration 12300 / 20000: loss 2.116237\n",
      "iteration 12400 / 20000: loss 2.073151\n",
      "iteration 12500 / 20000: loss 2.083766\n",
      "iteration 12600 / 20000: loss 2.069657\n",
      "iteration 12700 / 20000: loss 2.073188\n",
      "iteration 12800 / 20000: loss 2.099672\n",
      "iteration 12900 / 20000: loss 2.072905\n",
      "iteration 13000 / 20000: loss 2.056705\n",
      "iteration 13100 / 20000: loss 2.081636\n",
      "iteration 13200 / 20000: loss 2.108867\n",
      "iteration 13300 / 20000: loss 2.085715\n",
      "iteration 13400 / 20000: loss 2.124900\n",
      "iteration 13500 / 20000: loss 2.073246\n",
      "iteration 13600 / 20000: loss 2.110798\n",
      "iteration 13700 / 20000: loss 2.076717\n",
      "iteration 13800 / 20000: loss 2.141497\n",
      "iteration 13900 / 20000: loss 2.082414\n",
      "iteration 14000 / 20000: loss 2.106883\n",
      "iteration 14100 / 20000: loss 2.073359\n",
      "iteration 14200 / 20000: loss 2.138158\n",
      "iteration 14300 / 20000: loss 2.099966\n",
      "iteration 14400 / 20000: loss 2.142716\n",
      "iteration 14500 / 20000: loss 2.134597\n",
      "iteration 14600 / 20000: loss 2.143272\n",
      "iteration 14700 / 20000: loss 2.098194\n",
      "iteration 14800 / 20000: loss 2.138082\n",
      "iteration 14900 / 20000: loss 2.058638\n",
      "iteration 15000 / 20000: loss 2.131590\n",
      "iteration 15100 / 20000: loss 2.066174\n",
      "iteration 15200 / 20000: loss 2.117862\n",
      "iteration 15300 / 20000: loss 2.083014\n",
      "iteration 15400 / 20000: loss 2.082165\n",
      "iteration 15500 / 20000: loss 2.042480\n",
      "iteration 15600 / 20000: loss 2.102627\n",
      "iteration 15700 / 20000: loss 2.115037\n",
      "iteration 15800 / 20000: loss 2.120883\n",
      "iteration 15900 / 20000: loss 2.023502\n",
      "iteration 16000 / 20000: loss 2.050302\n",
      "iteration 16100 / 20000: loss 1.993217\n",
      "iteration 16200 / 20000: loss 2.046487\n",
      "iteration 16300 / 20000: loss 2.106392\n",
      "iteration 16400 / 20000: loss 2.076012\n",
      "iteration 16500 / 20000: loss 2.120800\n",
      "iteration 16600 / 20000: loss 2.106194\n",
      "iteration 16700 / 20000: loss 2.034523\n",
      "iteration 16800 / 20000: loss 2.073119\n",
      "iteration 16900 / 20000: loss 2.071386\n",
      "iteration 17000 / 20000: loss 2.100219\n",
      "iteration 17100 / 20000: loss 2.056613\n",
      "iteration 17200 / 20000: loss 2.084266\n",
      "iteration 17300 / 20000: loss 2.062572\n",
      "iteration 17400 / 20000: loss 2.017229\n",
      "iteration 17500 / 20000: loss 2.028515\n",
      "iteration 17600 / 20000: loss 2.058493\n",
      "iteration 17700 / 20000: loss 2.158338\n",
      "iteration 17800 / 20000: loss 2.112108\n",
      "iteration 17900 / 20000: loss 2.060769\n",
      "iteration 18000 / 20000: loss 2.088548\n",
      "iteration 18100 / 20000: loss 2.060274\n",
      "iteration 18200 / 20000: loss 2.033864\n",
      "iteration 18300 / 20000: loss 2.083791\n",
      "iteration 18400 / 20000: loss 2.082818\n",
      "iteration 18500 / 20000: loss 2.005341\n",
      "iteration 18600 / 20000: loss 2.072506\n",
      "iteration 18700 / 20000: loss 2.065088\n",
      "iteration 18800 / 20000: loss 2.104102\n",
      "iteration 18900 / 20000: loss 2.070731\n",
      "iteration 19000 / 20000: loss 2.064791\n",
      "iteration 19100 / 20000: loss 2.095652\n",
      "iteration 19200 / 20000: loss 2.142630\n",
      "iteration 19300 / 20000: loss 2.041169\n",
      "iteration 19400 / 20000: loss 2.062368\n",
      "iteration 19500 / 20000: loss 2.094539\n",
      "iteration 19600 / 20000: loss 2.145202\n",
      "iteration 19700 / 20000: loss 2.111674\n",
      "iteration 19800 / 20000: loss 2.110125\n",
      "iteration 19900 / 20000: loss 2.122571\n",
      "lr, rs:  1e-08 1000.0\n",
      "iteration 0 / 20000: loss 20.399752\n",
      "iteration 100 / 20000: loss 20.225654\n",
      "iteration 200 / 20000: loss 19.697369\n",
      "iteration 300 / 20000: loss 19.899092\n",
      "iteration 400 / 20000: loss 19.054672\n",
      "iteration 500 / 20000: loss 19.839502\n",
      "iteration 600 / 20000: loss 19.198815\n",
      "iteration 700 / 20000: loss 18.972712\n",
      "iteration 800 / 20000: loss 19.353464\n",
      "iteration 900 / 20000: loss 19.512570\n",
      "iteration 1000 / 20000: loss 18.963681\n",
      "iteration 1100 / 20000: loss 19.300649\n",
      "iteration 1200 / 20000: loss 18.645259\n",
      "iteration 1300 / 20000: loss 18.985322\n",
      "iteration 1400 / 20000: loss 18.615454\n",
      "iteration 1500 / 20000: loss 18.411536\n",
      "iteration 1600 / 20000: loss 18.601770\n",
      "iteration 1700 / 20000: loss 18.286966\n",
      "iteration 1800 / 20000: loss 18.423324\n",
      "iteration 1900 / 20000: loss 18.543139\n",
      "iteration 2000 / 20000: loss 17.848167\n",
      "iteration 2100 / 20000: loss 18.206845\n",
      "iteration 2200 / 20000: loss 17.938414\n",
      "iteration 2300 / 20000: loss 17.968461\n",
      "iteration 2400 / 20000: loss 17.907041\n",
      "iteration 2500 / 20000: loss 17.820424\n",
      "iteration 2600 / 20000: loss 17.925130\n",
      "iteration 2700 / 20000: loss 17.749169\n",
      "iteration 2800 / 20000: loss 17.751202\n",
      "iteration 2900 / 20000: loss 18.086487\n",
      "iteration 3000 / 20000: loss 17.960749\n",
      "iteration 3100 / 20000: loss 17.454041\n",
      "iteration 3200 / 20000: loss 17.810803\n",
      "iteration 3300 / 20000: loss 17.369775\n",
      "iteration 3400 / 20000: loss 17.458315\n",
      "iteration 3500 / 20000: loss 17.510701\n",
      "iteration 3600 / 20000: loss 17.326306\n",
      "iteration 3700 / 20000: loss 17.529238\n",
      "iteration 3800 / 20000: loss 17.408639\n",
      "iteration 3900 / 20000: loss 17.154364\n",
      "iteration 4000 / 20000: loss 17.363189\n",
      "iteration 4100 / 20000: loss 16.852987\n",
      "iteration 4200 / 20000: loss 17.358074\n",
      "iteration 4300 / 20000: loss 17.005777\n",
      "iteration 4400 / 20000: loss 16.925321\n",
      "iteration 4500 / 20000: loss 16.728277\n",
      "iteration 4600 / 20000: loss 16.869108\n",
      "iteration 4700 / 20000: loss 16.696633\n",
      "iteration 4800 / 20000: loss 16.900239\n",
      "iteration 4900 / 20000: loss 16.731211\n",
      "iteration 5000 / 20000: loss 16.620734\n",
      "iteration 5100 / 20000: loss 16.900245\n",
      "iteration 5200 / 20000: loss 16.790133\n",
      "iteration 5300 / 20000: loss 16.835560\n",
      "iteration 5400 / 20000: loss 16.440998\n",
      "iteration 5500 / 20000: loss 16.497559\n",
      "iteration 5600 / 20000: loss 16.728625\n",
      "iteration 5700 / 20000: loss 16.423755\n",
      "iteration 5800 / 20000: loss 16.320431\n",
      "iteration 5900 / 20000: loss 16.297773\n",
      "iteration 6000 / 20000: loss 16.497279\n",
      "iteration 6100 / 20000: loss 16.452462\n",
      "iteration 6200 / 20000: loss 16.300782\n",
      "iteration 6300 / 20000: loss 16.424921\n",
      "iteration 6400 / 20000: loss 16.102052\n",
      "iteration 6500 / 20000: loss 16.306494\n",
      "iteration 6600 / 20000: loss 15.997376\n",
      "iteration 6700 / 20000: loss 15.962921\n",
      "iteration 6800 / 20000: loss 15.977029\n",
      "iteration 6900 / 20000: loss 16.168225\n",
      "iteration 7000 / 20000: loss 15.872074\n",
      "iteration 7100 / 20000: loss 15.810901\n",
      "iteration 7200 / 20000: loss 16.099963\n",
      "iteration 7300 / 20000: loss 15.754468\n",
      "iteration 7400 / 20000: loss 15.655138\n",
      "iteration 7500 / 20000: loss 15.728283\n",
      "iteration 7600 / 20000: loss 15.889082\n",
      "iteration 7700 / 20000: loss 15.881395\n",
      "iteration 7800 / 20000: loss 15.893325\n",
      "iteration 7900 / 20000: loss 15.731662\n",
      "iteration 8000 / 20000: loss 15.744222\n",
      "iteration 8100 / 20000: loss 15.532848\n",
      "iteration 8200 / 20000: loss 15.706882\n",
      "iteration 8300 / 20000: loss 15.397679\n",
      "iteration 8400 / 20000: loss 15.718848\n",
      "iteration 8500 / 20000: loss 15.563941\n",
      "iteration 8600 / 20000: loss 15.369245\n",
      "iteration 8700 / 20000: loss 15.464703\n",
      "iteration 8800 / 20000: loss 15.345273\n",
      "iteration 8900 / 20000: loss 15.541448\n",
      "iteration 9000 / 20000: loss 15.206641\n",
      "iteration 9100 / 20000: loss 15.281820\n",
      "iteration 9200 / 20000: loss 15.195178\n",
      "iteration 9300 / 20000: loss 15.272652\n",
      "iteration 9400 / 20000: loss 15.325315\n",
      "iteration 9500 / 20000: loss 15.015867\n",
      "iteration 9600 / 20000: loss 15.106528\n",
      "iteration 9700 / 20000: loss 15.092996\n",
      "iteration 9800 / 20000: loss 15.206607\n",
      "iteration 9900 / 20000: loss 15.050178\n",
      "iteration 10000 / 20000: loss 14.880155\n",
      "iteration 10100 / 20000: loss 14.932350\n",
      "iteration 10200 / 20000: loss 15.034388\n",
      "iteration 10300 / 20000: loss 15.178881\n",
      "iteration 10400 / 20000: loss 14.729741\n",
      "iteration 10500 / 20000: loss 14.775865\n",
      "iteration 10600 / 20000: loss 14.785120\n",
      "iteration 10700 / 20000: loss 14.933642\n",
      "iteration 10800 / 20000: loss 14.723313\n",
      "iteration 10900 / 20000: loss 14.644554\n",
      "iteration 11000 / 20000: loss 14.861689\n",
      "iteration 11100 / 20000: loss 14.651575\n",
      "iteration 11200 / 20000: loss 14.584067\n",
      "iteration 11300 / 20000: loss 14.467306\n",
      "iteration 11400 / 20000: loss 14.652480\n",
      "iteration 11500 / 20000: loss 14.536500\n",
      "iteration 11600 / 20000: loss 14.517052\n",
      "iteration 11700 / 20000: loss 14.478106\n",
      "iteration 11800 / 20000: loss 14.458637\n",
      "iteration 11900 / 20000: loss 14.592451\n",
      "iteration 12000 / 20000: loss 14.435218\n",
      "iteration 12100 / 20000: loss 14.439017\n",
      "iteration 12200 / 20000: loss 14.346545\n",
      "iteration 12300 / 20000: loss 14.320857\n",
      "iteration 12400 / 20000: loss 14.327908\n",
      "iteration 12500 / 20000: loss 14.215961\n",
      "iteration 12600 / 20000: loss 14.068202\n",
      "iteration 12700 / 20000: loss 14.037889\n",
      "iteration 12800 / 20000: loss 14.358573\n",
      "iteration 12900 / 20000: loss 14.082417\n",
      "iteration 13000 / 20000: loss 14.166265\n",
      "iteration 13100 / 20000: loss 14.149497\n",
      "iteration 13200 / 20000: loss 14.097942\n",
      "iteration 13300 / 20000: loss 13.894985\n",
      "iteration 13400 / 20000: loss 13.917397\n",
      "iteration 13500 / 20000: loss 14.153463\n",
      "iteration 13600 / 20000: loss 13.958006\n",
      "iteration 13700 / 20000: loss 14.063830\n",
      "iteration 13800 / 20000: loss 13.779571\n",
      "iteration 13900 / 20000: loss 13.797367\n",
      "iteration 14000 / 20000: loss 13.830316\n",
      "iteration 14100 / 20000: loss 13.884609\n",
      "iteration 14200 / 20000: loss 13.777083\n",
      "iteration 14300 / 20000: loss 13.912687\n",
      "iteration 14400 / 20000: loss 13.795829\n",
      "iteration 14500 / 20000: loss 13.586692\n",
      "iteration 14600 / 20000: loss 13.615882\n",
      "iteration 14700 / 20000: loss 13.658264\n",
      "iteration 14800 / 20000: loss 13.674386\n",
      "iteration 14900 / 20000: loss 13.734081\n",
      "iteration 15000 / 20000: loss 13.596476\n",
      "iteration 15100 / 20000: loss 13.676294\n",
      "iteration 15200 / 20000: loss 13.575099\n",
      "iteration 15300 / 20000: loss 13.538728\n",
      "iteration 15400 / 20000: loss 13.402656\n",
      "iteration 15500 / 20000: loss 13.530904\n",
      "iteration 15600 / 20000: loss 13.456175\n",
      "iteration 15700 / 20000: loss 13.177645\n",
      "iteration 15800 / 20000: loss 13.341938\n",
      "iteration 15900 / 20000: loss 13.152108\n",
      "iteration 16000 / 20000: loss 13.413409\n",
      "iteration 16100 / 20000: loss 13.541766\n",
      "iteration 16200 / 20000: loss 13.498571\n",
      "iteration 16300 / 20000: loss 13.169552\n",
      "iteration 16400 / 20000: loss 13.216134\n",
      "iteration 16500 / 20000: loss 13.154832\n",
      "iteration 16600 / 20000: loss 13.287959\n",
      "iteration 16700 / 20000: loss 13.075960\n",
      "iteration 16800 / 20000: loss 12.978993\n",
      "iteration 16900 / 20000: loss 13.064204\n",
      "iteration 17000 / 20000: loss 12.911788\n",
      "iteration 17100 / 20000: loss 13.013766\n",
      "iteration 17200 / 20000: loss 13.049927\n",
      "iteration 17300 / 20000: loss 12.873797\n",
      "iteration 17400 / 20000: loss 13.000622\n",
      "iteration 17500 / 20000: loss 12.923590\n",
      "iteration 17600 / 20000: loss 12.825668\n",
      "iteration 17700 / 20000: loss 12.848540\n",
      "iteration 17800 / 20000: loss 12.782584\n",
      "iteration 17900 / 20000: loss 12.802012\n",
      "iteration 18000 / 20000: loss 12.951561\n",
      "iteration 18100 / 20000: loss 12.808564\n",
      "iteration 18200 / 20000: loss 12.663777\n",
      "iteration 18300 / 20000: loss 12.827576\n",
      "iteration 18400 / 20000: loss 12.578406\n",
      "iteration 18500 / 20000: loss 12.841971\n",
      "iteration 18600 / 20000: loss 12.634026\n",
      "iteration 18700 / 20000: loss 12.655171\n",
      "iteration 18800 / 20000: loss 12.554233\n",
      "iteration 18900 / 20000: loss 12.564286\n",
      "iteration 19000 / 20000: loss 12.603522\n",
      "iteration 19100 / 20000: loss 12.674002\n",
      "iteration 19200 / 20000: loss 12.440145\n",
      "iteration 19300 / 20000: loss 12.498897\n",
      "iteration 19400 / 20000: loss 12.587026\n",
      "iteration 19500 / 20000: loss 12.242346\n",
      "iteration 19600 / 20000: loss 12.431434\n",
      "iteration 19700 / 20000: loss 12.655765\n",
      "iteration 19800 / 20000: loss 12.516539\n",
      "iteration 19900 / 20000: loss 12.309522\n",
      "lr, rs:  1e-08 5000.0\n",
      "iteration 0 / 20000: loss 82.308556\n",
      "iteration 100 / 20000: loss 81.163708\n",
      "iteration 200 / 20000: loss 80.404324\n",
      "iteration 300 / 20000: loss 79.142316\n",
      "iteration 400 / 20000: loss 78.365727\n",
      "iteration 500 / 20000: loss 77.747599\n",
      "iteration 600 / 20000: loss 76.702430\n",
      "iteration 700 / 20000: loss 75.823565\n",
      "iteration 800 / 20000: loss 75.280388\n",
      "iteration 900 / 20000: loss 74.109163\n",
      "iteration 1000 / 20000: loss 73.196718\n",
      "iteration 1100 / 20000: loss 72.647933\n",
      "iteration 1200 / 20000: loss 72.003701\n",
      "iteration 1300 / 20000: loss 70.861517\n",
      "iteration 1400 / 20000: loss 70.524795\n",
      "iteration 1500 / 20000: loss 69.541876\n",
      "iteration 1600 / 20000: loss 68.988564\n",
      "iteration 1700 / 20000: loss 68.220630\n",
      "iteration 1800 / 20000: loss 67.546525\n",
      "iteration 1900 / 20000: loss 66.683065\n",
      "iteration 2000 / 20000: loss 66.021360\n",
      "iteration 2100 / 20000: loss 65.240416\n",
      "iteration 2200 / 20000: loss 64.837872\n",
      "iteration 2300 / 20000: loss 64.017365\n",
      "iteration 2400 / 20000: loss 63.673893\n",
      "iteration 2500 / 20000: loss 62.622448\n",
      "iteration 2600 / 20000: loss 62.216207\n",
      "iteration 2700 / 20000: loss 61.477169\n",
      "iteration 2800 / 20000: loss 60.705887\n",
      "iteration 2900 / 20000: loss 60.067184\n",
      "iteration 3000 / 20000: loss 59.732685\n",
      "iteration 3100 / 20000: loss 59.166530\n",
      "iteration 3200 / 20000: loss 58.622360\n",
      "iteration 3300 / 20000: loss 58.031507\n",
      "iteration 3400 / 20000: loss 57.402002\n",
      "iteration 3500 / 20000: loss 56.872839\n",
      "iteration 3600 / 20000: loss 56.395976\n",
      "iteration 3700 / 20000: loss 55.664949\n",
      "iteration 3800 / 20000: loss 55.238641\n",
      "iteration 3900 / 20000: loss 54.596357\n",
      "iteration 4000 / 20000: loss 54.020567\n",
      "iteration 4100 / 20000: loss 53.393683\n",
      "iteration 4200 / 20000: loss 53.035395\n",
      "iteration 4300 / 20000: loss 52.428094\n",
      "iteration 4400 / 20000: loss 52.055815\n",
      "iteration 4500 / 20000: loss 51.556712\n",
      "iteration 4600 / 20000: loss 50.896895\n",
      "iteration 4700 / 20000: loss 50.511382\n",
      "iteration 4800 / 20000: loss 49.846859\n",
      "iteration 4900 / 20000: loss 49.478939\n",
      "iteration 5000 / 20000: loss 48.877789\n",
      "iteration 5100 / 20000: loss 48.343610\n",
      "iteration 5200 / 20000: loss 47.976225\n",
      "iteration 5300 / 20000: loss 47.455397\n",
      "iteration 5400 / 20000: loss 46.969381\n",
      "iteration 5500 / 20000: loss 46.509882\n",
      "iteration 5600 / 20000: loss 46.068015\n",
      "iteration 5700 / 20000: loss 45.709387\n",
      "iteration 5800 / 20000: loss 45.264771\n",
      "iteration 5900 / 20000: loss 44.967020\n",
      "iteration 6000 / 20000: loss 44.433600\n",
      "iteration 6100 / 20000: loss 43.918780\n",
      "iteration 6200 / 20000: loss 43.498703\n",
      "iteration 6300 / 20000: loss 43.010349\n",
      "iteration 6400 / 20000: loss 42.429211\n",
      "iteration 6500 / 20000: loss 42.202670\n",
      "iteration 6600 / 20000: loss 41.674223\n",
      "iteration 6700 / 20000: loss 41.341633\n",
      "iteration 6800 / 20000: loss 41.180621\n",
      "iteration 6900 / 20000: loss 40.554879\n",
      "iteration 7000 / 20000: loss 40.186911\n",
      "iteration 7100 / 20000: loss 39.825133\n",
      "iteration 7200 / 20000: loss 39.535721\n",
      "iteration 7300 / 20000: loss 39.041659\n",
      "iteration 7400 / 20000: loss 38.873292\n",
      "iteration 7500 / 20000: loss 38.225937\n",
      "iteration 7600 / 20000: loss 37.931258\n",
      "iteration 7700 / 20000: loss 37.465500\n",
      "iteration 7800 / 20000: loss 37.232481\n",
      "iteration 7900 / 20000: loss 37.020211\n",
      "iteration 8000 / 20000: loss 36.478953\n",
      "iteration 8100 / 20000: loss 36.285363\n",
      "iteration 8200 / 20000: loss 35.721997\n",
      "iteration 8300 / 20000: loss 35.325977\n",
      "iteration 8400 / 20000: loss 35.123134\n",
      "iteration 8500 / 20000: loss 34.581742\n",
      "iteration 8600 / 20000: loss 34.537768\n",
      "iteration 8700 / 20000: loss 34.108259\n",
      "iteration 8800 / 20000: loss 33.668069\n",
      "iteration 8900 / 20000: loss 33.318849\n",
      "iteration 9000 / 20000: loss 32.984797\n",
      "iteration 9100 / 20000: loss 32.798878\n",
      "iteration 9200 / 20000: loss 32.551432\n",
      "iteration 9300 / 20000: loss 32.130407\n",
      "iteration 9400 / 20000: loss 31.828870\n",
      "iteration 9500 / 20000: loss 31.637074\n",
      "iteration 9600 / 20000: loss 31.390516\n",
      "iteration 9700 / 20000: loss 30.956157\n",
      "iteration 9800 / 20000: loss 30.707569\n",
      "iteration 9900 / 20000: loss 30.378058\n",
      "iteration 10000 / 20000: loss 30.167848\n",
      "iteration 10100 / 20000: loss 29.642308\n",
      "iteration 10200 / 20000: loss 29.597144\n",
      "iteration 10300 / 20000: loss 29.360837\n",
      "iteration 10400 / 20000: loss 29.095774\n",
      "iteration 10500 / 20000: loss 28.761904\n",
      "iteration 10600 / 20000: loss 28.314577\n",
      "iteration 10700 / 20000: loss 28.220272\n",
      "iteration 10800 / 20000: loss 27.897506\n",
      "iteration 10900 / 20000: loss 27.593154\n",
      "iteration 11000 / 20000: loss 27.420745\n",
      "iteration 11100 / 20000: loss 27.039552\n",
      "iteration 11200 / 20000: loss 26.944078\n",
      "iteration 11300 / 20000: loss 26.571314\n",
      "iteration 11400 / 20000: loss 26.326960\n",
      "iteration 11500 / 20000: loss 26.235949\n",
      "iteration 11600 / 20000: loss 25.777204\n",
      "iteration 11700 / 20000: loss 25.691352\n",
      "iteration 11800 / 20000: loss 25.392054\n",
      "iteration 11900 / 20000: loss 25.111451\n",
      "iteration 12000 / 20000: loss 24.865484\n",
      "iteration 12100 / 20000: loss 24.659556\n",
      "iteration 12200 / 20000: loss 24.652665\n",
      "iteration 12300 / 20000: loss 24.242685\n",
      "iteration 12400 / 20000: loss 23.984086\n",
      "iteration 12500 / 20000: loss 23.740708\n",
      "iteration 12600 / 20000: loss 23.535292\n",
      "iteration 12700 / 20000: loss 23.321218\n",
      "iteration 12800 / 20000: loss 22.946319\n",
      "iteration 12900 / 20000: loss 22.880154\n",
      "iteration 13000 / 20000: loss 22.740833\n",
      "iteration 13100 / 20000: loss 22.582296\n",
      "iteration 13200 / 20000: loss 22.245594\n",
      "iteration 13300 / 20000: loss 22.208623\n",
      "iteration 13400 / 20000: loss 21.850201\n",
      "iteration 13500 / 20000: loss 21.639048\n",
      "iteration 13600 / 20000: loss 21.587850\n",
      "iteration 13700 / 20000: loss 21.263095\n",
      "iteration 13800 / 20000: loss 21.124159\n",
      "iteration 13900 / 20000: loss 20.851940\n",
      "iteration 14000 / 20000: loss 20.632375\n",
      "iteration 14100 / 20000: loss 20.577587\n",
      "iteration 14200 / 20000: loss 20.406750\n",
      "iteration 14300 / 20000: loss 20.074016\n",
      "iteration 14400 / 20000: loss 19.862686\n",
      "iteration 14500 / 20000: loss 19.760753\n",
      "iteration 14600 / 20000: loss 19.534987\n",
      "iteration 14700 / 20000: loss 19.480366\n",
      "iteration 14800 / 20000: loss 19.202431\n",
      "iteration 14900 / 20000: loss 19.221400\n",
      "iteration 15000 / 20000: loss 18.807544\n",
      "iteration 15100 / 20000: loss 18.684233\n",
      "iteration 15200 / 20000: loss 18.563096\n",
      "iteration 15300 / 20000: loss 18.336832\n",
      "iteration 15400 / 20000: loss 18.140692\n",
      "iteration 15500 / 20000: loss 18.157195\n",
      "iteration 15600 / 20000: loss 17.835581\n",
      "iteration 15700 / 20000: loss 17.670275\n",
      "iteration 15800 / 20000: loss 17.694408\n",
      "iteration 15900 / 20000: loss 17.406570\n",
      "iteration 16000 / 20000: loss 17.212913\n",
      "iteration 16100 / 20000: loss 17.176515\n",
      "iteration 16200 / 20000: loss 16.956243\n",
      "iteration 16300 / 20000: loss 16.751014\n",
      "iteration 16400 / 20000: loss 16.521854\n",
      "iteration 16500 / 20000: loss 16.514609\n",
      "iteration 16600 / 20000: loss 16.490213\n",
      "iteration 16700 / 20000: loss 16.103634\n",
      "iteration 16800 / 20000: loss 16.153911\n",
      "iteration 16900 / 20000: loss 15.886902\n",
      "iteration 17000 / 20000: loss 15.809894\n",
      "iteration 17100 / 20000: loss 15.623789\n",
      "iteration 17200 / 20000: loss 15.383922\n",
      "iteration 17300 / 20000: loss 15.240000\n",
      "iteration 17400 / 20000: loss 15.134647\n",
      "iteration 17500 / 20000: loss 15.225904\n",
      "iteration 17600 / 20000: loss 14.904776\n",
      "iteration 17700 / 20000: loss 14.825866\n",
      "iteration 17800 / 20000: loss 14.624961\n",
      "iteration 17900 / 20000: loss 14.531320\n",
      "iteration 18000 / 20000: loss 14.400167\n",
      "iteration 18100 / 20000: loss 14.123107\n",
      "iteration 18200 / 20000: loss 14.148060\n",
      "iteration 18300 / 20000: loss 14.034320\n",
      "iteration 18400 / 20000: loss 14.006929\n",
      "iteration 18500 / 20000: loss 13.907106\n",
      "iteration 18600 / 20000: loss 13.787946\n",
      "iteration 18700 / 20000: loss 13.441798\n",
      "iteration 18800 / 20000: loss 13.457660\n",
      "iteration 18900 / 20000: loss 13.291760\n",
      "iteration 19000 / 20000: loss 13.253853\n",
      "iteration 19100 / 20000: loss 13.045195\n",
      "iteration 19200 / 20000: loss 13.004530\n",
      "iteration 19300 / 20000: loss 12.923398\n",
      "iteration 19400 / 20000: loss 12.825367\n",
      "iteration 19500 / 20000: loss 12.639149\n",
      "iteration 19600 / 20000: loss 12.499690\n",
      "iteration 19700 / 20000: loss 12.451371\n",
      "iteration 19800 / 20000: loss 12.393028\n",
      "iteration 19900 / 20000: loss 12.258583\n",
      "lr, rs:  1e-08 50000.0\n",
      "iteration 0 / 20000: loss 773.047765\n",
      "iteration 100 / 20000: loss 699.281543\n",
      "iteration 200 / 20000: loss 632.567368\n",
      "iteration 300 / 20000: loss 572.120335\n",
      "iteration 400 / 20000: loss 517.863659\n",
      "iteration 500 / 20000: loss 468.458458\n",
      "iteration 600 / 20000: loss 423.624462\n",
      "iteration 700 / 20000: loss 383.386811\n",
      "iteration 800 / 20000: loss 346.862525\n",
      "iteration 900 / 20000: loss 314.149530\n",
      "iteration 1000 / 20000: loss 284.375686\n",
      "iteration 1100 / 20000: loss 257.431157\n",
      "iteration 1200 / 20000: loss 233.017066\n",
      "iteration 1300 / 20000: loss 211.069924\n",
      "iteration 1400 / 20000: loss 190.942806\n",
      "iteration 1500 / 20000: loss 173.029375\n",
      "iteration 1600 / 20000: loss 156.595015\n",
      "iteration 1700 / 20000: loss 142.046966\n",
      "iteration 1800 / 20000: loss 128.584241\n",
      "iteration 1900 / 20000: loss 116.440032\n",
      "iteration 2000 / 20000: loss 105.557612\n",
      "iteration 2100 / 20000: loss 95.619003\n",
      "iteration 2200 / 20000: loss 86.787878\n",
      "iteration 2300 / 20000: loss 78.624211\n",
      "iteration 2400 / 20000: loss 71.270628\n",
      "iteration 2500 / 20000: loss 64.770617\n",
      "iteration 2600 / 20000: loss 58.837404\n",
      "iteration 2700 / 20000: loss 53.351824\n",
      "iteration 2800 / 20000: loss 48.471744\n",
      "iteration 2900 / 20000: loss 44.024442\n",
      "iteration 3000 / 20000: loss 39.975130\n",
      "iteration 3100 / 20000: loss 36.407045\n",
      "iteration 3200 / 20000: loss 33.082627\n",
      "iteration 3300 / 20000: loss 30.259695\n",
      "iteration 3400 / 20000: loss 27.523265\n",
      "iteration 3500 / 20000: loss 25.059435\n",
      "iteration 3600 / 20000: loss 22.813600\n",
      "iteration 3700 / 20000: loss 20.907267\n",
      "iteration 3800 / 20000: loss 19.059638\n",
      "iteration 3900 / 20000: loss 17.428358\n",
      "iteration 4000 / 20000: loss 16.007049\n",
      "iteration 4100 / 20000: loss 14.719648\n",
      "iteration 4200 / 20000: loss 13.438996\n",
      "iteration 4300 / 20000: loss 12.387965\n",
      "iteration 4400 / 20000: loss 11.399998\n",
      "iteration 4500 / 20000: loss 10.586853\n",
      "iteration 4600 / 20000: loss 9.722416\n",
      "iteration 4700 / 20000: loss 8.958360\n",
      "iteration 4800 / 20000: loss 8.337426\n",
      "iteration 4900 / 20000: loss 7.722469\n",
      "iteration 5000 / 20000: loss 7.168410\n",
      "iteration 5100 / 20000: loss 6.717783\n",
      "iteration 5200 / 20000: loss 6.229619\n",
      "iteration 5300 / 20000: loss 5.804539\n",
      "iteration 5400 / 20000: loss 5.484072\n",
      "iteration 5500 / 20000: loss 5.157166\n",
      "iteration 5600 / 20000: loss 4.884918\n",
      "iteration 5700 / 20000: loss 4.585965\n",
      "iteration 5800 / 20000: loss 4.373259\n",
      "iteration 5900 / 20000: loss 4.146297\n",
      "iteration 6000 / 20000: loss 4.008490\n",
      "iteration 6100 / 20000: loss 3.819070\n",
      "iteration 6200 / 20000: loss 3.685948\n",
      "iteration 6300 / 20000: loss 3.454595\n",
      "iteration 6400 / 20000: loss 3.381425\n",
      "iteration 6500 / 20000: loss 3.290752\n",
      "iteration 6600 / 20000: loss 3.134748\n",
      "iteration 6700 / 20000: loss 3.096410\n",
      "iteration 6800 / 20000: loss 2.881227\n",
      "iteration 6900 / 20000: loss 2.819044\n",
      "iteration 7000 / 20000: loss 2.760484\n",
      "iteration 7100 / 20000: loss 2.753867\n",
      "iteration 7200 / 20000: loss 2.646668\n",
      "iteration 7300 / 20000: loss 2.581123\n",
      "iteration 7400 / 20000: loss 2.581515\n",
      "iteration 7500 / 20000: loss 2.482399\n",
      "iteration 7600 / 20000: loss 2.500437\n",
      "iteration 7700 / 20000: loss 2.389522\n",
      "iteration 7800 / 20000: loss 2.297493\n",
      "iteration 7900 / 20000: loss 2.407451\n",
      "iteration 8000 / 20000: loss 2.380961\n",
      "iteration 8100 / 20000: loss 2.322697\n",
      "iteration 8200 / 20000: loss 2.310373\n",
      "iteration 8300 / 20000: loss 2.291566\n",
      "iteration 8400 / 20000: loss 2.256114\n",
      "iteration 8500 / 20000: loss 2.209785\n",
      "iteration 8600 / 20000: loss 2.226973\n",
      "iteration 8700 / 20000: loss 2.208151\n",
      "iteration 8800 / 20000: loss 2.186285\n",
      "iteration 8900 / 20000: loss 2.187553\n",
      "iteration 9000 / 20000: loss 2.106327\n",
      "iteration 9100 / 20000: loss 2.129295\n",
      "iteration 9200 / 20000: loss 2.194574\n",
      "iteration 9300 / 20000: loss 2.161514\n",
      "iteration 9400 / 20000: loss 2.161692\n",
      "iteration 9500 / 20000: loss 2.119699\n",
      "iteration 9600 / 20000: loss 2.111054\n",
      "iteration 9700 / 20000: loss 2.098301\n",
      "iteration 9800 / 20000: loss 2.093606\n",
      "iteration 9900 / 20000: loss 2.183847\n",
      "iteration 10000 / 20000: loss 2.097211\n",
      "iteration 10100 / 20000: loss 2.158101\n",
      "iteration 10200 / 20000: loss 2.095539\n",
      "iteration 10300 / 20000: loss 2.075184\n",
      "iteration 10400 / 20000: loss 2.112413\n",
      "iteration 10500 / 20000: loss 2.078728\n",
      "iteration 10600 / 20000: loss 2.096886\n",
      "iteration 10700 / 20000: loss 2.130191\n",
      "iteration 10800 / 20000: loss 2.115464\n",
      "iteration 10900 / 20000: loss 2.102284\n",
      "iteration 11000 / 20000: loss 2.088589\n",
      "iteration 11100 / 20000: loss 2.013470\n",
      "iteration 11200 / 20000: loss 2.123914\n",
      "iteration 11300 / 20000: loss 2.092435\n",
      "iteration 11400 / 20000: loss 2.087340\n",
      "iteration 11500 / 20000: loss 2.062089\n",
      "iteration 11600 / 20000: loss 2.062256\n",
      "iteration 11700 / 20000: loss 2.110911\n",
      "iteration 11800 / 20000: loss 2.075878\n",
      "iteration 11900 / 20000: loss 2.116460\n",
      "iteration 12000 / 20000: loss 2.088136\n",
      "iteration 12100 / 20000: loss 2.097198\n",
      "iteration 12200 / 20000: loss 2.110309\n",
      "iteration 12300 / 20000: loss 2.095106\n",
      "iteration 12400 / 20000: loss 2.066663\n",
      "iteration 12500 / 20000: loss 2.063195\n",
      "iteration 12600 / 20000: loss 2.086702\n",
      "iteration 12700 / 20000: loss 1.968393\n",
      "iteration 12800 / 20000: loss 2.074764\n",
      "iteration 12900 / 20000: loss 2.079677\n",
      "iteration 13000 / 20000: loss 2.124256\n",
      "iteration 13100 / 20000: loss 2.113650\n",
      "iteration 13200 / 20000: loss 2.079244\n",
      "iteration 13300 / 20000: loss 2.105852\n",
      "iteration 13400 / 20000: loss 2.097627\n",
      "iteration 13500 / 20000: loss 2.120120\n",
      "iteration 13600 / 20000: loss 2.023086\n",
      "iteration 13700 / 20000: loss 2.044265\n",
      "iteration 13800 / 20000: loss 2.062503\n",
      "iteration 13900 / 20000: loss 2.107149\n",
      "iteration 14000 / 20000: loss 2.109772\n",
      "iteration 14100 / 20000: loss 2.114572\n",
      "iteration 14200 / 20000: loss 2.054709\n",
      "iteration 14300 / 20000: loss 2.054908\n",
      "iteration 14400 / 20000: loss 2.074237\n",
      "iteration 14500 / 20000: loss 2.034554\n",
      "iteration 14600 / 20000: loss 2.022242\n",
      "iteration 14700 / 20000: loss 2.085264\n",
      "iteration 14800 / 20000: loss 2.130505\n",
      "iteration 14900 / 20000: loss 2.031445\n",
      "iteration 15000 / 20000: loss 2.082597\n",
      "iteration 15100 / 20000: loss 2.070489\n",
      "iteration 15200 / 20000: loss 2.046470\n",
      "iteration 15300 / 20000: loss 2.121922\n",
      "iteration 15400 / 20000: loss 2.121945\n",
      "iteration 15500 / 20000: loss 2.066990\n",
      "iteration 15600 / 20000: loss 2.119100\n",
      "iteration 15700 / 20000: loss 2.085897\n",
      "iteration 15800 / 20000: loss 2.161459\n",
      "iteration 15900 / 20000: loss 2.126851\n",
      "iteration 16000 / 20000: loss 2.014741\n",
      "iteration 16100 / 20000: loss 2.062806\n",
      "iteration 16200 / 20000: loss 2.049621\n",
      "iteration 16300 / 20000: loss 1.999849\n",
      "iteration 16400 / 20000: loss 2.078091\n",
      "iteration 16500 / 20000: loss 2.054833\n",
      "iteration 16600 / 20000: loss 2.099350\n",
      "iteration 16700 / 20000: loss 2.132229\n",
      "iteration 16800 / 20000: loss 2.050742\n",
      "iteration 16900 / 20000: loss 2.056060\n",
      "iteration 17000 / 20000: loss 2.062960\n",
      "iteration 17100 / 20000: loss 2.049525\n",
      "iteration 17200 / 20000: loss 2.053899\n",
      "iteration 17300 / 20000: loss 2.039412\n",
      "iteration 17400 / 20000: loss 2.061713\n",
      "iteration 17500 / 20000: loss 2.081059\n",
      "iteration 17600 / 20000: loss 2.095403\n",
      "iteration 17700 / 20000: loss 2.101696\n",
      "iteration 17800 / 20000: loss 2.067198\n",
      "iteration 17900 / 20000: loss 2.050510\n",
      "iteration 18000 / 20000: loss 2.130593\n",
      "iteration 18100 / 20000: loss 2.052292\n",
      "iteration 18200 / 20000: loss 2.077346\n",
      "iteration 18300 / 20000: loss 2.044734\n",
      "iteration 18400 / 20000: loss 2.090150\n",
      "iteration 18500 / 20000: loss 2.120680\n",
      "iteration 18600 / 20000: loss 2.026878\n",
      "iteration 18700 / 20000: loss 2.073394\n",
      "iteration 18800 / 20000: loss 2.164447\n",
      "iteration 18900 / 20000: loss 2.066260\n",
      "iteration 19000 / 20000: loss 2.004616\n",
      "iteration 19100 / 20000: loss 2.096901\n",
      "iteration 19200 / 20000: loss 2.058351\n",
      "iteration 19300 / 20000: loss 2.092860\n",
      "iteration 19400 / 20000: loss 2.105530\n",
      "iteration 19500 / 20000: loss 2.084342\n",
      "iteration 19600 / 20000: loss 2.068216\n",
      "iteration 19700 / 20000: loss 2.084815\n",
      "iteration 19800 / 20000: loss 2.031480\n",
      "iteration 19900 / 20000: loss 2.047867\n",
      "lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.275122 val accuracy: 0.267000\n",
      "lr 1.000000e-08 reg 5.000000e+03 train accuracy: 0.321490 val accuracy: 0.333000\n",
      "lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.330306 val accuracy: 0.340000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.412673 val accuracy: 0.415000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.389714 val accuracy: 0.408000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.329469 val accuracy: 0.342000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.416122 val accuracy: 0.414000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.388673 val accuracy: 0.388000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.323082 val accuracy: 0.331000\n",
      "best validation accuracy achieved during cross-validation: 0.415000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7, 1e-8]\n",
    "regularization_strengths = [1e3, 5e3, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "import itertools\n",
    "\n",
    "for lr, rs in itertools.product(learning_rates, regularization_strengths):\n",
    "    print 'lr, rs: ', lr, rs\n",
    "    \n",
    "    softmax = Softmax()\n",
    "    loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                          num_iters=20000, verbose=True)\n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    training_accuracy = np.mean(y_train == y_train_pred)\n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    validation_accuracy = np.mean(y_val == y_val_pred) \n",
    "    results[(lr,rs)] = (training_accuracy, validation_accuracy)\n",
    "    \n",
    "    if validation_accuracy > best_val: \n",
    "        best_val = validation_accuracy\n",
    "        best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.379000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAF/CAYAAABQVS1eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+IE8PAmCIAiCIPjBfwDuuBFaIcZ4BQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1104602d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
